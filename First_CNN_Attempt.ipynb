{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "First_CNN_Attempt.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJtCeCgbSs0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this notebook is based off of this blog post: \n",
        "# https://realpython.com/python-keras-text-classification/#reader-comments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QYIGfoPFRZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASET_NAME = \"test\"\n",
        "MODEL_NAME = \"test\"\n",
        "MAX_SEQ_LENGTH = 20\n",
        "TRAINING_SET_SIZE = 1000000\n",
        "VAL_SET_SIZE = 1000000\n",
        "NUM_EPOCHS = 15\n",
        "BATCH_SIZE = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0rrZSx7Ss0M",
        "colab_type": "code",
        "outputId": "c834a814-914b-442b-bbb4-79b386ac7c79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from scipy.sparse import hstack, vstack\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import Sequential, layers\n",
        "from keras.utils import plot_model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "import time\n",
        "import pickle\n",
        "!pip install gcsfs\n",
        "\n",
        "pd.set_option('max_colwidth', 100)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting gcsfs\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/92/0297f2813cb240c52e90f8587420149970565800e019e1b08ef5ad28b6d9/gcsfs-0.3.1.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▋                        | 10kB 19.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.4.2)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.21.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.1)\n",
            "Requirement already satisfied: fsspec>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.7)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (3.1.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.12.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.2.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n",
            "Building wheels for collected packages: gcsfs\n",
            "  Building wheel for gcsfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gcsfs: filename=gcsfs-0.3.1-py2.py3-none-any.whl size=17936 sha256=4b577dbe95e8323e390331908a987c3bff220b808c15196dc2c2fd023e528132\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/2b/6f/86954f0d8caa1173841e62bb780dc0f8693bd268e04a267682\n",
            "Successfully built gcsfs\n",
            "Installing collected packages: gcsfs\n",
            "Successfully installed gcsfs-0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKNfUKrfUgP-",
        "colab_type": "code",
        "outputId": "93fc6dda-5759-4830-8d9d-b4326b78345b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# this cell is only necessary if running in colab\n",
        "project_id = 'w266-251323'\n",
        "import uuid\n",
        "bucket_name = 'fb-congressional-data/'\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcloud config set project {project_id}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T7PzdLQSs0j",
        "colab_type": "code",
        "outputId": "54d393f8-6429-44f8-f5e6-6213cb42a87a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "train_df = pd.read_csv(\"gs://fb-congressional-data/train.csv\", index_col=0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iOniVLISs0m",
        "colab_type": "code",
        "outputId": "23d65830-59ab-48e2-d9f7-325338fe288c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "dev_df = pd.read_csv(\"gs://fb-congressional-data/dev.csv\", index_col=0)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgZWntiySs0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shuffle the data\n",
        "# be sure to do this before you extract X's and y's!!\n",
        "train_df = train_df.sample(frac=1)\n",
        "dev_df = dev_df.sample(frac=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjb6dx8fSs0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = train_df.op_gender.values\n",
        "y_dev = dev_df.op_gender.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBCoPKh_Ss0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def turn_to_ints(li):\n",
        "    final_list = []\n",
        "    for gender in li:\n",
        "        if gender=='M':\n",
        "            final_list.append(1)\n",
        "        else:\n",
        "            final_list.append(0)\n",
        "    return final_list\n",
        "            \n",
        "y_train = turn_to_ints(y_train)\n",
        "y_dev = turn_to_ints(y_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN9GFz6xSs0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = np.asarray(y_train)\n",
        "y_dev = np.asarray(y_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChYxh6oPSs04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_text_list(init_list):\n",
        "    sentences = []\n",
        "    for sentence in init_list:\n",
        "        if type(sentence) != str:\n",
        "            sentences.append(\"\")\n",
        "        else:\n",
        "            sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "new_sentences_train = get_text_list(train_df.response_text.values)\n",
        "new_sentences_test = get_text_list(dev_df.response_text.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b9SE7SjSs06",
        "colab_type": "code",
        "outputId": "e49da5d7-57a5-47d7-9216-bb32a3af2ad0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "time_start = time.time()\n",
        "\n",
        "tokenizer = Tokenizer(num_words=200000)\n",
        "tokenizer.fit_on_texts(new_sentences_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(new_sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(new_sentences_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "currentTime = time.gmtime(time.time() - time_start)\n",
        "\n",
        "#Convert the gmtime struct to a string\n",
        "timeStr = time.strftime(\"%M minutes, %S seconds\", currentTime)\n",
        "\n",
        "print(\"Tokenized in {}\".format(timeStr))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized in 09 minutes, 56 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyZMBAiySs0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 20\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGl1S3HySs1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_path = 'X_train_{}.pkl'.format(DATASET_NAME)\n",
        "x_dev_path = 'X_dev_{}.pkl'.format(DATASET_NAME)\n",
        "y_train_path = 'y_train_{}.pkl'.format(DATASET_NAME)\n",
        "y_dev_path = 'y_dev_{}.pkl'.format(DATASET_NAME)\n",
        "\n",
        "with open(x_train_path, 'wb') as file:\n",
        "    pickle.dump(X_train, file)   \n",
        "with open(x_dev_path, 'wb') as file:\n",
        "    pickle.dump(X_test, file)\n",
        "with open(y_train_path, 'wb') as file:\n",
        "    pickle.dump(y_train, file)\n",
        "with open(y_dev_path, 'wb') as file:\n",
        "    pickle.dump(y_dev, file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hw2vmYxEWvg",
        "colab_type": "code",
        "outputId": "f910e574-f162-4774-a1d1-bd7b6788ccb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# copy to bucket\n",
        "!gsutil cp /content/{x_train_path} gs://fb-congressional-data/test\n",
        "!gsutil cp /content/{x_dev_path} gs://fb-congressional-data/\n",
        "!gsutil cp /content/{y_train_path} gs://fb-congressional-data/\n",
        "!gsutil cp /content/{y_dev_path} gs://fb-congressional-data/"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file:///content/X_train_test.pkl...\n",
            "-\n",
            "Operation completed over 1 objects/753.7 MiB.                                    \n",
            "Copying file:///content/X_dev_test.pkl [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "/\n",
            "Operation completed over 1 objects/174.9 MiB.                                    \n",
            "Copying file:///content/y_train_test.pkl [Content-Type=application/octet-stream]...\n",
            "\\\n",
            "Operation completed over 1 objects/75.4 MiB.                                     \n",
            "Copying file:///content/y_dev_test.pkl [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/17.5 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5AeCFGIuKIR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "51a9161b-b356-416b-a7cf-3a030307fd12"
      },
      "source": [
        "!gsutil cp gs://fb-congressional-data/glove* /tmp/"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://fb-congressional-data/glove.6B.100d.txt...\n",
            "Copying gs://fb-congressional-data/glove.6B.200d.txt...\n",
            "Copying gs://fb-congressional-data/glove.6B.300d.txt...\n",
            "Copying gs://fb-congressional-data/glove.6B.50d.txt...\n",
            "\\ [4 files][  2.1 GiB/  2.1 GiB]  100.2 MiB/s                                   \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://fb-congressional-data/glove.6B.zip...\n",
            "/ [5 files][  2.9 GiB/  2.9 GiB]    9.6 MiB/s                                   \n",
            "Operation completed over 5 objects/2.9 GiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae5EQ7vbSs1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_dim = 100\n",
        "embedding_matrix = create_embedding_matrix(\n",
        "                     '/tmp/glove.6B.{}d.txt'.format(embedding_dim),\n",
        "                      tokenizer.word_index, embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUTvp7yfut-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e529dbd2-67bc-475b-e866-46dbbcec3b8c"
      },
      "source": [
        "# trying to figure out which words are empty here\n",
        "counter = 0\n",
        "empty_indexes = []\n",
        "for index, row in enumerate(embedding_matrix):\n",
        "  if sum(row) == 0:\n",
        "    empty_indexes.append(index)\n",
        "    counter += 1\n",
        "  if counter > 1000:\n",
        "    break\n",
        "\n",
        "for idx in empty_indexes:\n",
        "  try:\n",
        "    print(tokenizer.index_word[idx])\n",
        "  except:\n",
        "    print(\"No entry for {}\".format(idx))\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No entry for 0\n",
            "don't\n",
            "it's\n",
            "i'm\n",
            "can't\n",
            "that's\n",
            "you're\n",
            "doesn't\n",
            "didn't\n",
            "he's\n",
            "won't\n",
            "isn't\n",
            "let's\n",
            "what's\n",
            "obama's\n",
            "i've\n",
            "we're\n",
            "aren't\n",
            "wouldn't\n",
            "i'll\n",
            "they're\n",
            "she's\n",
            "i'd\n",
            "you've\n",
            "there's\n",
            "wasn't\n",
            "haven't\n",
            "shouldn't\n",
            "couldn't\n",
            "you'll\n",
            "stillsanders\n",
            "trump's\n",
            "youtu\n",
            "fbid\n",
            "we've\n",
            "palestinei\n",
            "here's\n",
            "hasn't\n",
            "we'll\n",
            "people's\n",
            "america's\n",
            "gov't\n",
            "women's\n",
            "•\n",
            "who's\n",
            "you'd\n",
            "president's\n",
            "clinton's\n",
            "god's\n",
            "weren't\n",
            "standwithrand\n",
            "country's\n",
            "they've\n",
            "nation's\n",
            "y'all\n",
            "hillary's\n",
            "they'll\n",
            "children's\n",
            "🇺🇸\n",
            "he'll\n",
            "today's\n",
            "obummer\n",
            "ain't\n",
            "rinos\n",
            "state's\n",
            "where's\n",
            "🇺🇸🇺🇸\n",
            "doyourjob\n",
            "huffingtonpost\n",
            "bush's\n",
            "american's\n",
            "we'd\n",
            "👍\n",
            "washingtonpost\n",
            "feelthebern\n",
            "😍\n",
            "lmao\n",
            "woman's\n",
            "party's\n",
            "one's\n",
            "government's\n",
            "world's\n",
            "killary\n",
            "everyone's\n",
            "❤️\n",
            "he'd\n",
            "paul's\n",
            "they'd\n",
            "fbcdn\n",
            "man's\n",
            "father's\n",
            "mother's\n",
            "else's\n",
            "family's\n",
            "nobillnobreak\n",
            "administration's\n",
            "hphotos\n",
            "hadn't\n",
            "bridenstine\n",
            "it'll\n",
            "randrally\n",
            "hahaha\n",
            "she'll\n",
            "how's\n",
            "isil\n",
            "randpaul\n",
            "cispa\n",
            "bengazi\n",
            "iran's\n",
            "😊\n",
            "bernie's\n",
            "'the\n",
            "sheeple\n",
            "israel's\n",
            "someone's\n",
            "bernieorbust\n",
            "😂\n",
            "would've\n",
            "veteran's\n",
            "rino's\n",
            "neverhillary\n",
            "reid's\n",
            "aspx\n",
            "republican's\n",
            "😡\n",
            "ryan's\n",
            "ya'll\n",
            "thehill\n",
            "1073741828\n",
            "repub\n",
            "warren's\n",
            "person's\n",
            "gmo's\n",
            "dailycaller\n",
            "trump2016\n",
            "libtards\n",
            "thinkprogress\n",
            "anyone's\n",
            "boehner's\n",
            "§\n",
            "congress'\n",
            "gop's\n",
            "rand's\n",
            "citizen's\n",
            "scontent\n",
            "obozo\n",
            "year's\n",
            "senator's\n",
            "͡°\n",
            "❤\n",
            "husband's\n",
            "dem's\n",
            "should've\n",
            "company's\n",
            "politicususa\n",
            "berniesanders\n",
            "child's\n",
            "reagan's\n",
            "agsaf\n",
            "westernjournalism\n",
            "╬═╬\n",
            "sphotos\n",
            "states'\n",
            "devos'\n",
            "60's\n",
            "2366\n",
            "70's\n",
            "👎\n",
            "washingtontimes\n",
            "worstresponders\n",
            "drumpf\n",
            "hahahaha\n",
            "fref\n",
            "she'd\n",
            "teabaggers\n",
            "nobody's\n",
            "akamaihd\n",
            "russia's\n",
            "alzheimer's\n",
            "ceo's\n",
            "demandavote\n",
            "law's\n",
            "wenstrup\n",
            "gowdy's\n",
            "son's\n",
            "libtard\n",
            "hilliary\n",
            "80's\n",
            "hero's\n",
            "teaparty\n",
            "banksters\n",
            "koolaid\n",
            "wife's\n",
            "oboma\n",
            "romney's\n",
            "'we\n",
            "nationalreview\n",
            "life's\n",
            "senate's\n",
            "infowars\n",
            "notif\n",
            "😍😍😍\n",
            "sayin'\n",
            "nobama\n",
            "ovomit\n",
            "get'em\n",
            "shouldnt\n",
            "🇺🇸🇺🇸🇺🇸\n",
            "bernie2016\n",
            "veterans'\n",
            "congress's\n",
            "protectsacredlands\n",
            "opensecrets\n",
            "occupydemocrats\n",
            "r's\n",
            "😍😍\n",
            "daca\n",
            "washington's\n",
            "2fwww\n",
            "men's\n",
            "other's\n",
            "nodapl\n",
            "imho\n",
            "pelosi's\n",
            "mcconnell's\n",
            "devoss\n",
            "♥\n",
            "who've\n",
            "bill's\n",
            "washingtonexaminer\n",
            "driver's\n",
            "dad's\n",
            "democrat's\n",
            "king's\n",
            "dumbass\n",
            "china's\n",
            "daughter's\n",
            "congressman's\n",
            "wtg\n",
            "could've\n",
            "cruz's\n",
            "citizens'\n",
            "that'll\n",
            "dapl\n",
            "qur'an\n",
            "███\n",
            "people'\n",
            "govtrack\n",
            "0bama\n",
            "على\n",
            "cnsnews\n",
            "cromnibus\n",
            "conservativetribune\n",
            "putin's\n",
            "😉\n",
            "taxpayer's\n",
            "killery\n",
            "gofundme\n",
            "lieing\n",
            "😀\n",
            "obuma\n",
            "🙏\n",
            "benifits\n",
            "amodei\n",
            "democraps\n",
            "democrates\n",
            "bucshon\n",
            "p71bjjcp8\n",
            "♡\n",
            "randpaul2016\n",
            "sanders'\n",
            "brown's\n",
            "california's\n",
            "bonamici\n",
            "americans'\n",
            "felicidades\n",
            "court's\n",
            "abcnews\n",
            "2646\n",
            "guy's\n",
            "netanyahu's\n",
            "someunknownushistory\n",
            "addictinginfo\n",
            "pnref\n",
            "90's\n",
            "sotu\n",
            "theguardian\n",
            "tahmooressi\n",
            "🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸\n",
            "͜ʖ\n",
            "😂😂😂\n",
            "rawstory\n",
            "dailymail\n",
            "50's\n",
            "peoples'\n",
            "motherjones\n",
            "doctor's\n",
            "imwithher\n",
            "sthash\n",
            "devos's\n",
            "douchebag\n",
            "businessinsider\n",
            "nevertrump\n",
            "committee's\n",
            "rediculous\n",
            "d's\n",
            "syria's\n",
            "mcdonald's\n",
            "repugs\n",
            "latimes\n",
            "😢\n",
            "mom's\n",
            "vermont's\n",
            "bday\n",
            "speach\n",
            "teabagger\n",
            "kxl\n",
            "baby's\n",
            "djt\n",
            "house's\n",
            "istandwithrand\n",
            "chemtrails\n",
            "ijreview\n",
            "noflynobuy\n",
            "department's\n",
            "nbcnews\n",
            "public's\n",
            "awsome\n",
            "soooooo\n",
            "freebeacon\n",
            "stopp\n",
            "republicans'\n",
            "cbsnews\n",
            "holdthefloor\n",
            "em'\n",
            "obummercare\n",
            "it'd\n",
            "constituents'\n",
            "thegatewaypundit\n",
            "assad's\n",
            "thanx\n",
            "taxpayers'\n",
            "issa's\n",
            "sessions'\n",
            "comey's\n",
            "kobanê\n",
            "beforeitsnews\n",
            "obamacare's\n",
            "😂😂\n",
            "permalink\n",
            "freedomoutpost\n",
            "obamanation\n",
            "governor's\n",
            "holder's\n",
            "romneycare\n",
            "'bout\n",
            "1073741825\n",
            "tinyurl\n",
            "demoncrats\n",
            "hahahahaha\n",
            "لا\n",
            "usnews\n",
            "democrap\n",
            "cliven\n",
            "valentine's\n",
            "alqaeda\n",
            "😃\n",
            "presidentpaul\n",
            "constituants\n",
            "hitler's\n",
            "navient\n",
            "▓▓▓▓\n",
            "tpnn\n",
            "hr1942\n",
            "deplorables\n",
            "demorats\n",
            "talkingpointsmemo\n",
            "parents'\n",
            "dpuf\n",
            "fed's\n",
            "😎\n",
            "parent's\n",
            "╚═\n",
            "═╝\n",
            "o's\n",
            "conservativereview\n",
            "ocare\n",
            "yesterday's\n",
            "it'\n",
            "representative's\n",
            "kerry's\n",
            "obamatrade\n",
            "everyones\n",
            "'no'\n",
            "passabill\n",
            "mediamatters\n",
            "politician's\n",
            "2fwatch\n",
            "timeisnow\n",
            "'i\n",
            "mccain's\n",
            "nusra\n",
            "nazi's\n",
            "fbi's\n",
            "🙄\n",
            "democrats'\n",
            "😄\n",
            "schumer's\n",
            "100's\n",
            "sheriff's\n",
            "'16\n",
            "dumbasses\n",
            "'no\n",
            "redflagnews\n",
            "kate's\n",
            "kinzinger\n",
            "bengahzi\n",
            "2207520000\n",
            "agency's\n",
            "😁\n",
            "kennedy's\n",
            "shillary\n",
            "employee's\n",
            "weaselzippers\n",
            "shits\n",
            "therightscoop\n",
            "theatlantic\n",
            "harm's\n",
            "3fv\n",
            "brother's\n",
            "hitlery\n",
            "💙\n",
            "mftrand\n",
            "teacher's\n",
            "idk\n",
            "sheep's\n",
            "individual's\n",
            "allenbwest\n",
            "'er\n",
            "nypost\n",
            "vaping\n",
            "　\n",
            "wmd's\n",
            "gunviolencearchive\n",
            "bannon's\n",
            "hr2366\n",
            "michigan's\n",
            "john's\n",
            "gdata\n",
            "freepalestine\n",
            "smdh\n",
            "s1214\n",
            "epa's\n",
            "obumer\n",
            "lord's\n",
            "😭\n",
            "gov'ts\n",
            "renewui\n",
            "346937065399354\n",
            "americanthinker\n",
            "jcpoa\n",
            "everybody's\n",
            "dailysignal\n",
            "ncid\n",
            "ww3\n",
            "😳\n",
            "its'\n",
            "illegal's\n",
            "elibiary\n",
            "2381\n",
            "1960's\n",
            "katko\n",
            "city's\n",
            "al's\n",
            "primaried\n",
            "vet's\n",
            "monsanto's\n",
            "womenforrandpaul2016\n",
            "foodstamps\n",
            "jesus'\n",
            "1950's\n",
            "comming\n",
            "dipshit\n",
            "week's\n",
            "military's\n",
            "💜\n",
            "nbwbd8jcjgy\n",
            "franni\n",
            "florida's\n",
            "democrate\n",
            "friend's\n",
            "don'\n",
            "❤️❤️❤️\n",
            "rouzer\n",
            "palestinesupport\n",
            "🤔\n",
            "██\n",
            "bullcrap\n",
            "franken's\n",
            "rubio's\n",
            "night's\n",
            "naturalnews\n",
            "get's\n",
            "workers'\n",
            "thedailybeast\n",
            "😘\n",
            "👍🏻\n",
            "o'bama\n",
            "☆\n",
            "hilary's\n",
            "donaldtrump\n",
            "hr2646\n",
            "fillibuster\n",
            "freakin'\n",
            "weeklystandard\n",
            "o'\n",
            "1073741826\n",
            "media's\n",
            "1073741829\n",
            "1970's\n",
            "scalia's\n",
            "☺\n",
            "christ's\n",
            "parenthood's\n",
            "hippa\n",
            "alaska's\n",
            "demexit\n",
            "uscode\n",
            "1980's\n",
            "cmon\n",
            "👏\n",
            "turds\n",
            "minnesota's\n",
            "familys\n",
            "'t\n",
            "ar15\n",
            "1930's\n",
            "makedclisten\n",
            "left's\n",
            "va's\n",
            "foundation's\n",
            "dcclothesline\n",
            "billmoyers\n",
            "hiliary\n",
            "auditthefed\n",
            "😔\n",
            "general's\n",
            "notmypresident\n",
            "thenewamerican\n",
            "11e6\n",
            "teapublicans\n",
            "biden's\n",
            "oorah\n",
            "carolina's\n",
            "id's\n",
            "•i\n",
            "😞\n",
            "💕\n",
            "employer's\n",
            "lerner's\n",
            "grassley's\n",
            "obamacares\n",
            "ummmm\n",
            "saudi's\n",
            "freegaza\n",
            "asshat\n",
            "berniesanders2016\n",
            "mexico's\n",
            "😍😍😍😍\n",
            "odumbo\n",
            "gopthedailydose\n",
            "murray's\n",
            "koch's\n",
            "business's\n",
            "asshats\n",
            "sander's\n",
            "barry's\n",
            "bullsh\n",
            "12mj\n",
            "u76nfg\n",
            "earth's\n",
            "canada's\n",
            "want's\n",
            "ttip\n",
            "job's\n",
            "kentucky's\n",
            "😠\n",
            "1073741827\n",
            "nra's\n",
            "youve\n",
            "1000's\n",
            "qid\n",
            "347907068635687\n",
            "81180\n",
            "😒\n",
            "warren2020\n",
            "rep's\n",
            "how'd\n",
            "daesh\n",
            "others'\n",
            "unvetted\n",
            "medicade\n",
            "right's\n",
            "lobby's\n",
            "heaven's\n",
            "tipsheet\n",
            "wonkblog\n",
            "tulsi2020\n",
            "master's\n",
            "pete's\n",
            "bernies\n",
            "act's\n",
            "worker's\n",
            "hillary2016\n",
            "tonight's\n",
            "kids'\n",
            "،\n",
            "10's\n",
            "uslicensee\n",
            "😕\n",
            "ان\n",
            "constitution's\n",
            "americasfreedomfighters\n",
            "kid's\n",
            "donald's\n",
            "makeamericagreatagain\n",
            "somebody's\n",
            "nixon's\n",
            "perpetrator's\n",
            "dumbocrats\n",
            "xpf1\n",
            "zerohedge\n",
            "emailid\n",
            "selfie\n",
            "lee's\n",
            "rickungar\n",
            "moochers\n",
            "day's\n",
            "who'd\n",
            "congradulations\n",
            "attorney's\n",
            "☺️\n",
            "nydailynews\n",
            "dispicable\n",
            "another's\n",
            "hillaryclinton\n",
            "reince\n",
            "that'd\n",
            "notmeus\n",
            "longlivepalestinelonglivegaza\n",
            "liers\n",
            "11e3\n",
            "401k's\n",
            "voter's\n",
            "istandwithstandingrock\n",
            "494877883886870\n",
            "🙏🏻\n",
            "prn2\n",
            "frontpagemag\n",
            "xpa1\n",
            "awwwww\n",
            "'a\n",
            "egypt's\n",
            "obamcare\n",
            "aca's\n",
            "candidate's\n",
            "judge's\n",
            "your's\n",
            "caintv\n",
            "fbact\n",
            "boehners\n",
            "'you\n",
            "████\n",
            "bizpacreview\n",
            "lady's\n",
            "llmuks\n",
            "hypocrit\n",
            "utah's\n",
            "devil's\n",
            "nsa's\n",
            "bundy's\n",
            "سوريا\n",
            "🎄\n",
            "theintercept\n",
            "defunds\n",
            "christian's\n",
            "intcmp\n",
            "starr's\n",
            "deductable\n",
            "obumma\n",
            "dr's\n",
            "ash3\n",
            "newyorker\n",
            "💪💪💪💪💪💪💪💪💪💪\n",
            "usuncut\n",
            "founder's\n",
            "f'n\n",
            "globalresearch\n",
            "county's\n",
            "oregon's\n",
            "judicialwatch\n",
            "blacklivesmatter\n",
            "dontfundobamacare\n",
            "usa's\n",
            "🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸\n",
            "valadao\n",
            "grandchildren's\n",
            "burr's\n",
            "cbslocal\n",
            "hotair\n",
            "rat's\n",
            "😜\n",
            "say's\n",
            "kihuen\n",
            "illegals'\n",
            "universalfreepress\n",
            "commondreams\n",
            "demonrats\n",
            "kynect\n",
            "😩\n",
            "business'\n",
            "democracynow\n",
            "barton's\n",
            "gowdys\n",
            "republicons\n",
            "bandaid\n",
            "when's\n",
            "tennessee's\n",
            "officialhcec\n",
            "30's\n",
            "joe's\n",
            "obummers\n",
            "alll\n",
            "rofl\n",
            "jainhemraj59\n",
            "johnson's\n",
            "scott's\n",
            "presidentbyamendment\n",
            "scarey\n",
            "money's\n",
            "daddy's\n",
            "enemy's\n",
            "sooooooo\n",
            "hubbel\n",
            "🇺🇸🇺🇸🇺🇸🇺🇸\n",
            "group's\n",
            "middleeast\n",
            "elizabeth's\n",
            "gtfo\n",
            "satan's\n",
            "speaker's\n",
            "detailpage\n",
            "feinstein's\n",
            "dieing\n",
            "rand2016\n",
            "💖\n",
            "00pm\n",
            "turbyfill\n",
            "unliked\n",
            "werent\n",
            "amash's\n",
            "crooksandliars\n",
            "liberal's\n",
            "nutjob\n",
            "articleid\n",
            "refid\n",
            "1990's\n",
            "fricken\n",
            "hahahahahaha\n",
            "shintos\n",
            "😆\n",
            "noooooo\n",
            "hcec\n",
            "lookin'\n",
            "bo's\n",
            "🎉\n",
            "●hitler\n",
            "hrc's\n",
            "👍👍👍\n",
            "imgur\n",
            "parkinson's\n",
            "lyin'\n",
            "somthing\n",
            "f'ing\n",
            "ya'\n",
            "georgia's\n",
            "40's\n",
            "obamma\n",
            "opps\n",
            "thefederalist\n",
            "graham's\n",
            "hbd\n",
            "dickhead\n",
            "👍🏼\n",
            "أن\n",
            "dosent\n",
            "intrest\n",
            "😱\n",
            "16's\n",
            "fkisrael\n",
            "😂😂😂😂\n",
            "blackburn's\n",
            "votesmart\n",
            "republicrats\n",
            "gazaunderattack\n",
            "akid\n",
            "dumptrump\n",
            "dnc's\n",
            "repukes\n",
            "389658314427637\n",
            "dhupchaya\n",
            "protidin\n",
            "inkilab\n",
            "vorer\n",
            "kagog\n",
            "bankster\n",
            "💩\n",
            "you'\n",
            "polosi\n",
            "defundobamacare\n",
            "flotus\n",
            "ira's\n",
            "patient's\n",
            "fricking\n",
            "👏👏👏\n",
            "401034789956656\n",
            "90394\n",
            "controll\n",
            "asswipe\n",
            "nothing's\n",
            "booo\n",
            "cheney's\n",
            "church's\n",
            "fdr's\n",
            "🙈\n",
            "mitch's\n",
            "fool's\n",
            "students'\n",
            "lincoln's\n",
            "thenation\n",
            "mediaite\n",
            "republicians\n",
            "1800's\n",
            "obummer's\n",
            "seperation\n",
            "gettem\n",
            "dropouthillary\n",
            "😇\n",
            "elizabethwarren\n",
            "trey's\n",
            "8301\n",
            "truthandaction\n",
            "lifenews\n",
            "tomorrow's\n",
            "👍👍\n",
            "❤️❤️\n",
            "countries'\n",
            "batshit\n",
            "\b\n",
            "street's\n",
            "randgirling\n",
            "jillnothill\n",
            "likeshow\n",
            "greatful\n",
            "admendment\n",
            "ted's\n",
            "republicon\n",
            "thst\n",
            "disarmhate\n",
            "employees'\n",
            "upworthy\n",
            "chicagotribune\n",
            "cheeto\n",
            "'betrayal\n",
            "hres620\n",
            "xap1\n",
            "anyones\n",
            "michelle's\n",
            "homenews\n",
            "flynn's\n",
            "⦁\n",
            "society's\n",
            "mchenry's\n",
            "see's\n",
            "americanism'\n",
            "cajones\n",
            "'in\n",
            "soldier's\n",
            "selfs\n",
            "benifit\n",
            "treasonist\n",
            "barackobama\n",
            "disapointed\n",
            "aattp\n",
            "carter's\n",
            "demorat\n",
            "alqaida\n",
            "derp\n",
            "foxbusiness\n",
            "absolutly\n",
            "thepoliticalinsider\n",
            "refugee's\n",
            "york's\n",
            "usdebtclock\n",
            "companies'\n",
            "politians\n",
            "lundergan\n",
            "'free\n",
            "muslim's\n",
            "3rds\n",
            "us's\n",
            "thefederalistpapers\n",
            "hmmmmmm\n",
            "dhimmitude\n",
            "cking\n",
            "cnn's\n",
            "colorado's\n",
            "rightwingnews\n",
            "reemployamerica\n",
            "irandeal\n",
            "forwardprogressives\n",
            "nationaljournal\n",
            "removeboehner\n",
            "who'll\n",
            "dosen't\n",
            "boooo\n",
            "month's\n",
            "1sw1r\n",
            "'yes'\n",
            "frameborder\n",
            "school's\n",
            "208195102528120\n",
            "geeze\n",
            "doodee\n",
            "i'\n",
            "up'\n",
            "friggin'\n",
            "gomert\n",
            "geesh\n",
            "bachelor's\n",
            "lynch's\n",
            "kellogg's\n",
            "imigration\n",
            "boener\n",
            "stillranding\n",
            "terroist\n",
            "shitting\n",
            "demoncrat\n",
            "draintheswamp\n",
            "pakistanchristianpost\n",
            "supportzadroga2015\n",
            "yessss\n",
            "upfiles2\n",
            "nonimmigrants\n",
            "counrty\n",
            "'if\n",
            "gohmert's\n",
            "harry's\n",
            "visa's\n",
            "maloula\n",
            "iowa's\n",
            "industry's\n",
            "�\n",
            "youngcons\n",
            "patrick's\n",
            "polititions\n",
            "loosers\n",
            "republicants\n",
            "sob's\n",
            "obammy\n",
            "pagewanted\n",
            "neighbor's\n",
            "'what\n",
            "markwayne\n",
            "wapo\n",
            "unfollow\n",
            "'good\n",
            "socialflow\n",
            "wethepeople\n",
            "💗\n",
            "gov't's\n",
            "palin's\n",
            "endthefed\n",
            "mnsure\n",
            "allowfullscreen\n",
            "alassad\n",
            "utf8\n",
            "castro's\n",
            "عن\n",
            "leader's\n",
            "boy's\n",
            "hr25\n",
            "pelosis\n",
            "hahah\n",
            "11e5\n",
            "payer's\n",
            "notodevos\n",
            "hellary\n",
            "hummm\n",
            "roberts'\n",
            "cia's\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI5vn5zBSs1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hmmmm....\n",
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "nonzero_elements / vocab_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJtNke4FSs1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb_yr2R9Ss1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=False))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_9KWVWHAAE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smaller_X_train = X_train[:TRAINING_SET_SIZE]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6gn7HvuAGgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smaller_y_train = y_train[:TRAINING_SET_SIZE]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf5R6UpGDW_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smaller_X_dev = X_test[:VAL_SET_SIZE]\n",
        "smaller_y_dev = y_dev[:VAL_SET_SIZE]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4dbwlaxSs1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  time_start = time.time()\n",
        "\n",
        "  history = model.fit(smaller_X_train, smaller_y_train,\n",
        "                      epochs=NUM_EPOCHS,\n",
        "                      verbose=True,\n",
        "                      validation_data=(smaller_X_dev, smaller_y_dev),\n",
        "                      batch_size=BATCH_SIZE)\n",
        "\n",
        "  currentTime = time.gmtime(time.time() - time_start)\n",
        "\n",
        "  #Convert the gmtime struct to a string\n",
        "  timeStr = time.strftime(\"%M minutes, %S seconds\", currentTime)\n",
        "\n",
        "  print(\"Trained in {}\".format(timeStr))\n",
        "\n",
        "except:\n",
        "  currentTime = time.gmtime(time.time() - time_start)\n",
        "\n",
        "  #Convert the gmtime struct to a string\n",
        "  timeStr = time.strftime(\"%M minutes, %S seconds\", currentTime)\n",
        "\n",
        "  print(\"Trained in {}\".format(timeStr))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4P5Y5wgSs1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(smaller_X_train, smaller_y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_dev, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5G9XjaGJh-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this will only work if you finish the training...hmm \n",
        "plot_model(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccL7n659KbQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}