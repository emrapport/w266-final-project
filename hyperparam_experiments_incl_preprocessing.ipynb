{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperparam_experiments_incl_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emrapport/w266-final-project/blob/master/hyperparam_experiments_incl_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVfEbx0cZNNp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zHeoGlIX2sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# name of subfolder under models/ where trained models should go\n",
        "MODEL_PREFIX = \"test2\"\n",
        "\n",
        "hyp_combos = [{'NUM_EPOCHS': 1,\n",
        "                'BATCH_SIZE': 1000,\n",
        "                'MAX_SEQUENCE_LENGTH': 20,\n",
        "                'N_MOST_FREQ_WORDS_TO_KEEP': 5000,\n",
        "                'MAX_RESPONSES_PER_POST': 50,\n",
        "                'NUM_LAYERS': 2,\n",
        "                'CONV_LAYER_SIZES': [128, 64],\n",
        "                'FILTER_SIZES': [3, 2],\n",
        "                'DROPOUT_RATES': [.2, .2],\n",
        "                'FINAL_DENSE_LAYER_SIZE': 10,\n",
        "                # embed dim needs to map to one of the glove versions: 50, 100, 200, 300 \n",
        "                'EMBEDDING_DIM': 50,\n",
        "                'REMOVE_SHORT_RESP_LENGTH':2,\n",
        "                'SAMPLE_FROM_BEG_AND_END':False},\n",
        "              \n",
        "                {'NUM_EPOCHS': 1,\n",
        "                'BATCH_SIZE': 1000,\n",
        "                'MAX_SEQUENCE_LENGTH': 20,\n",
        "                'N_MOST_FREQ_WORDS_TO_KEEP': 5000,\n",
        "                'MAX_RESPONSES_PER_POST': 10,\n",
        "                'NUM_LAYERS': 3,\n",
        "                'CONV_LAYER_SIZES': [128, 64, 32],\n",
        "                'FILTER_SIZES': [3, 2, 3],\n",
        "                'DROPOUT_RATES': [.2, .2, .2],\n",
        "                'FINAL_DENSE_LAYER_SIZE': 10,\n",
        "                # embed dim needs to map to one of the glove versions: 50, 100, 200, 300 \n",
        "                'EMBEDDING_DIM': 50,\n",
        "                'REMOVE_SHORT_RESP_LENGTH':2,\n",
        "                'SAMPLE_FROM_BEG_AND_END':False}\n",
        "                ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGfRSm8JdMvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## check to ensure hyps are in agreement\n",
        "for hyp_combo in hyp_combos:\n",
        "  assert hyp_combo['NUM_LAYERS'] == len(hyp_combo['CONV_LAYER_SIZES'])\n",
        "  assert hyp_combo['NUM_LAYERS'] == len(hyp_combo['FILTER_SIZES'])\n",
        "  assert hyp_combo['NUM_LAYERS'] == len(hyp_combo['DROPOUT_RATES'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s79t_aJZsYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "f5bef526-973e-49d3-a01d-8362c81c74b8"
      },
      "source": [
        "## all this stuff just needs to get run one time per notebook\n",
        "# Set seeds for reproducible results.\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from scipy.sparse import hstack, vstack\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import Sequential, layers\n",
        "from keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "import time\n",
        "import pickle\n",
        "!pip install gcsfs\n",
        "\n",
        "pd.set_option('max_colwidth', 100)\n",
        "\n",
        "project_id = 'w266-251323'\n",
        "import uuid\n",
        "bucket_name = 'fb-congressional-data/'\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcloud config set project {project_id}\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting gcsfs\n",
            "  Downloading https://files.pythonhosted.org/packages/65/e2/05f903ce8f77804127195cfcc1ca8b500a3157a1572dcc4b82bf5af01564/gcsfs-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.1)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.1)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.21.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.7)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.12.0)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (3.1.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.7)\n",
            "Installing collected packages: gcsfs\n",
            "Successfully installed gcsfs-0.4.0\n",
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OugAe-Vx04Pf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "091e04dc-8630-4903-fe20-12f735013abd"
      },
      "source": [
        "train_df = pd.read_csv(\"gs://fb-congressional-data/contraction_expanded_data/train.csv\", index_col=0)\n",
        "dev_df = pd.read_csv(\"gs://fb-congressional-data/contraction_expanded_data/dev.csv\", index_col=0)\n",
        "!gsutil cp gs://fb-congressional-data/glove* /tmp/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://fb-congressional-data/glove.6B.100d.txt...\n",
            "Copying gs://fb-congressional-data/glove.6B.200d.txt...\n",
            "Copying gs://fb-congressional-data/glove.6B.300d.txt...\n",
            "Copying gs://fb-congressional-data/glove.6B.50d.txt...\n",
            "- [4 files][  2.1 GiB/  2.1 GiB]   55.4 MiB/s                                   \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://fb-congressional-data/glove.6B.zip...\n",
            "/ [5 files][  2.9 GiB/  2.9 GiB]   90.7 MiB/s                                   \n",
            "Operation completed over 5 objects/2.9 GiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysaWA5e_aGLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all the functions go here\n",
        "def remove_excess_rows_per_post(df, max_per_post):\n",
        "  num_responses_per_post = df.post_id.value_counts().reset_index()\n",
        "  num_responses_per_post.columns = ['post_id', 'num_responses']\n",
        "  \n",
        "  too_big_posts = num_responses_per_post[num_responses_per_post.num_responses > max_per_post]\n",
        "  posts_to_sample = too_big_posts.post_id.values\n",
        "  \n",
        "  # this gets all the rows for posts we DON'T need to sample \n",
        "  new_train_df = df[~df.post_id.isin(posts_to_sample)]\n",
        "  # this should be true\n",
        "  assert(len(too_big_posts) + new_train_df.post_id.nunique() == df.post_id.nunique())\n",
        "  \n",
        "  too_big_post_rows = df[df.post_id.isin(posts_to_sample)]\n",
        "  sampled_rows = too_big_post_rows.groupby('post_id').apply(lambda x: x.sample(n=max_per_post)).reset_index(drop=True)\n",
        "  new_train_df = pd.concat([new_train_df, sampled_rows])\n",
        "  \n",
        "  return new_train_df\n",
        "\n",
        "def get_labels(train_df, test_df, party_label_ind):\n",
        "\n",
        "  def turn_to_ints(li):\n",
        "    final_list = []\n",
        "    for gender in li:\n",
        "        if gender=='M':\n",
        "            final_list.append(1)\n",
        "        else:\n",
        "            final_list.append(0)\n",
        "    return final_list\n",
        "\n",
        "  def turn_to_ints_party(li):\n",
        "    final_list = []\n",
        "    for party in li:\n",
        "        if party=='Congress_Republican':\n",
        "            final_list.append(1)\n",
        "        else:\n",
        "            final_list.append(0)\n",
        "    return final_list\n",
        "\n",
        "  if party_label_ind:\n",
        "\n",
        "    y_train = train_df.op_category.values\n",
        "    y_dev = test_df.op_category.values\n",
        "    y_train = turn_to_ints_party(y_train)\n",
        "    y_dev = turn_to_ints_party(y_dev) \n",
        "\n",
        "  else:\n",
        "    y_train = train_df.op_gender.values\n",
        "    y_dev = test_df.op_gender.values\n",
        "    y_train = turn_to_ints(y_train)\n",
        "    y_dev = turn_to_ints(y_dev)\n",
        "\n",
        "  y_train = np.asarray(y_train)\n",
        "  y_dev = np.asarray(y_dev)\n",
        "\n",
        "  return y_train, y_dev\n",
        "\n",
        "def get_inputs(train_df, \n",
        "               test_df, \n",
        "               party_train_df,\n",
        "               party_dev_df,\n",
        "               n_words_to_keep,\n",
        "               max_seq_length):\n",
        "  def get_text_list(init_list):\n",
        "      sentences = []\n",
        "      for sentence in init_list:\n",
        "          if type(sentence) != str:\n",
        "              sentences.append(\"\")\n",
        "          else:\n",
        "              sentences.append(sentence)\n",
        "      return sentences\n",
        "\n",
        "  new_sentences_train = get_text_list(train_df.response_text.values)\n",
        "  new_sentences_test = get_text_list(dev_df.response_text.values)\n",
        "  party_new_sentences_train = get_text_list(party_train_df.response_text.values)\n",
        "  party_new_sentences_test = get_text_list(party_dev_df.response_text.values)\n",
        "\n",
        "  time_start = time.time()\n",
        "\n",
        "  # this is the default list of filters + apostrophe\n",
        "  # added because we have dealt with common contractions, so other apostrophes should mostly be possessive \n",
        "  tokenizer = Tokenizer(filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', oov_token='UNK')\n",
        "  tokenizer.fit_on_texts(new_sentences_train)\n",
        "\n",
        "\n",
        "  currentTime = time.gmtime(time.time() - time_start)\n",
        "\n",
        "  #Convert the gmtime struct to a string\n",
        "  timeStr = time.strftime(\"%M minutes, %S seconds\", currentTime)\n",
        "\n",
        "  print(\"Tokenized in {}\".format(timeStr))\n",
        "\n",
        "  # suggestion from this issue: https://github.com/keras-team/keras/issues/8092\n",
        "  # seems like OOV and num_words don't work correctly by default \n",
        "  tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() \n",
        "                              if i <= n_words_to_keep + 1} \n",
        "  tokenizer.word_index[tokenizer.oov_token] = n_words_to_keep + 1\n",
        "\n",
        "  X_train = tokenizer.texts_to_sequences(new_sentences_train)\n",
        "  X_test = tokenizer.texts_to_sequences(new_sentences_test)\n",
        "  X_train = pad_sequences(X_train, padding='post', maxlen=max_seq_length)\n",
        "  X_test = pad_sequences(X_test, padding='post', maxlen=max_seq_length)\n",
        "\n",
        "  X_train_party = tokenizer.texts_to_sequences(party_new_sentences_train)\n",
        "  X_test_party = tokenizer.texts_to_sequences(party_new_sentences_test)\n",
        "  X_train_party = pad_sequences(X_train_party, padding='post', maxlen=max_seq_length)\n",
        "  X_test_party = pad_sequences(X_test_party, padding='post', maxlen=max_seq_length)\n",
        "  return X_train, X_test, X_train_party, X_test_party, tokenizer\n",
        "\n",
        "def create_embedding_matrix(filepath, \n",
        "                            word_index, \n",
        "                            embedding_dim):\n",
        "    vocab_size = len(word_index) + 2  # Now we have to add 2 (reserved 0 plus the manual UNK token)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "def make_model(embedding_matrix, \n",
        "               max_seq_length,\n",
        "               num_layers,\n",
        "               conv_layer_size,\n",
        "               filter_size,\n",
        "               dropout_rate,\n",
        "               final_hidden_dense_size):\n",
        "  model = Sequential()\n",
        "\n",
        "  try:\n",
        "    model.add(layers.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], \n",
        "                              weights=[embedding_matrix], \n",
        "                              input_length=max_seq_length, \n",
        "                              trainable=False))\n",
        "    for i in range(num_layers):\n",
        "      model.add(layers.Conv1D(conv_layer_size[i], filter_size[i], activation='relu', padding=\"same\"))\n",
        "      model.add(layers.Dropout(dropout_rate[i]))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(final_hidden_dense_size, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "  except Exception as ex:\n",
        "    print(ex)\n",
        "  finally:\n",
        "    return model\n",
        "\n",
        "def train_model(model, \n",
        "                train_inputs,\n",
        "                dev_inputs,\n",
        "                train_labels,\n",
        "                dev_labels,\n",
        "                num_epochs,\n",
        "                batch_size):\n",
        "  try:\n",
        "    time_start = time.time()\n",
        "\n",
        "    history = model.fit(train_inputs, train_labels,\n",
        "                        epochs=num_epochs,\n",
        "                        verbose=True,\n",
        "                        validation_data=(dev_inputs, dev_labels),\n",
        "                        batch_size=batch_size)\n",
        "\n",
        "    currentTime = time.gmtime(time.time() - time_start)\n",
        "\n",
        "    #Convert the gmtime struct to a string\n",
        "    timeStr = time.strftime(\"%M minutes, %S seconds\", currentTime)\n",
        "\n",
        "    print(\"Trained in {}\".format(timeStr))\n",
        "\n",
        "  except Exception as ex:\n",
        "    print(\"Exception: {}\".format(ex))\n",
        "    currentTime = time.gmtime(time.time() - time_start)\n",
        "\n",
        "    #Convert the gmtime struct to a string\n",
        "    timeStr = time.strftime(\"%M minutes, %S seconds\", currentTime)\n",
        "\n",
        "    print(\"Trained in {}\".format(timeStr))  \n",
        "  finally:\n",
        "    return model\n",
        "\n",
        "def pred_to_label(row):\n",
        "  if row['probs'] >= .5:\n",
        "    return 'M'\n",
        "  else:\n",
        "    return 'W'\n",
        "\n",
        "def pred_to_party_label(row):\n",
        "  if row['probs2'] >= .5:\n",
        "    return 'Congress_Republican'\n",
        "  else:\n",
        "    return 'Not_Republican'\n",
        "\n",
        "def remove_short_responses(responses_df,n):\n",
        "  '''Takes a dataframe that includes a response_text column and removes \n",
        "  responses shorter than or equal to n. Returns new dataframe.'''\n",
        "  responses_df['split_response'] = responses_df['response_text'].str.split()\n",
        "  mask = (responses_df['split_response'].str.len() > n)\n",
        "  responses_df = responses_df.loc[mask]\n",
        "  return responses_df\n",
        "\n",
        "def shorten_single_response(series_list,length=20):\n",
        "  '''Take a list of strings and a goal max length. Return the goal length list\n",
        "  taken half from the beginning of the orginal list and half from the end of \n",
        "  the original list.'''\n",
        "  if len(series_list) > length:\n",
        "    new_list = series_list[:length//2] + series_list[-length//2:]\n",
        "    return new_list\n",
        "  else:\n",
        "    return series_list\n",
        "\n",
        "def get_beg_end_responses(responses_df,n):\n",
        "  '''Takes a dataframe that includes a response_text column, creates new\n",
        "  response_text strings of word length n using the first n/2 words and last n/2\n",
        "  words of the response_text and returns a new dataframe with the new response_text.'''\n",
        "  responses_df['split_response'] = responses_df['response_text'].str.split()\n",
        "  responses_df['short_response'] = responses_df['split_response'].apply(shorten_single_response,args=(n,))\n",
        "  responses_df['response_text'] = responses_df['short_response'].str.join(' ')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s844qbPgZi2s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1ef2e2f-a284-4b10-c817-89f6fa9d059f"
      },
      "source": [
        "# timestamp is shared across all runs\n",
        "timestamp = time.time()\n",
        "for i, hyp_dict in enumerate(hyp_combos):\n",
        "  print(\"Training Model {}\".format(i))\n",
        "  new_train_df = remove_excess_rows_per_post(train_df, hyp_dict['MAX_RESPONSES_PER_POST'])\n",
        "  print(\"Limiting by responses per post resulted in {} training rows\".format(new_train_df.shape[0]))\n",
        "  ########################################\n",
        "  # For testing; remove in final.\n",
        "  new_train_df = new_train_df[:10000]\n",
        "  ########################################\n",
        "  if hyp_dict['REMOVE_SHORT_RESP_LENGTH'] > 0:\n",
        "    remove_short_responses(new_train_df,hyp_dict['REMOVE_SHORT_RESP_LENGTH'])\n",
        "    remove_short_responses(dev_df,hyp_dict['REMOVE_SHORT_RESP_LENGTH'])\n",
        "    print(\"Removing responses under {} words resulted in {} training rows\".format((hyp_dict['REMOVE_SHORT_RESP_LENGTH']+1),new_train_df.shape[0]))\n",
        "\n",
        "    ######### Add if using test data###########\n",
        "    # remove_short_responses(test_df,hyp_dict['REMOVE_SHORT_RESP_LENGTH'])\n",
        "    ###########################################\n",
        "  if hyp_dict['SAMPLE_FROM_BEG_AND_END']:\n",
        "    get_beg_end_responses(new_train_df,hyp_dict['MAX_SEQUENCE_LENGTH'])\n",
        "    get_beg_end_responses(dev_df,hyp_dict['MAX_SEQUENCE_LENGTH'])\n",
        "    ######### Add if using test data###########\n",
        "    # get_beg_end_responses(test_df,hyp_dict['MAX_SEQUENCE_LENGTH'])\n",
        "    ###########################################\n",
        "    print(\"Responses include the first {} and last {} words\").format(hyp_dict['MAX_SEQUENCE_LENGTH']//2,hyp_dict['MAX_SEQUENCE_LENGTH']//2)\n",
        "\n",
        "\n",
        "  new_train_df = new_train_df.sample(frac=1)\n",
        "  dev_df = dev_df.sample(frac=1)\n",
        "\n",
        "\n",
        "  party_train_df = new_train_df[new_train_df.op_category!='Congress_Independent']\n",
        "  party_dev_df = dev_df[dev_df.op_category!='Congress_Independent']\n",
        "\n",
        "\n",
        "  # get two sets of labels: gender and party\n",
        "  print(\"Getting labels\")\n",
        "  y_train_gender, y_dev_gender = get_labels(new_train_df, dev_df, False)\n",
        "  y_train_party, y_dev_party = get_labels(party_train_df, party_dev_df, True)\n",
        "\n",
        "  print(\"Getting inputs\")\n",
        "  X_train, X_test, X_train_party, X_test_party, tokenizer = get_inputs(new_train_df, \n",
        "                                                                       dev_df, \n",
        "                                                                       party_train_df,\n",
        "                                                                       party_dev_df,\n",
        "                                                                       hyp_dict['N_MOST_FREQ_WORDS_TO_KEEP'], \n",
        "                                                                       hyp_dict['MAX_SEQUENCE_LENGTH'])\n",
        "  embedding_matrix = create_embedding_matrix(\n",
        "                     '/tmp/glove.6B.{}d.txt'.format(hyp_dict['EMBEDDING_DIM']),\n",
        "                      tokenizer.word_index, hyp_dict['EMBEDDING_DIM'])\n",
        "  \n",
        "  # gender model\n",
        "  model = make_model(embedding_matrix, \n",
        "                     hyp_dict['MAX_SEQUENCE_LENGTH'],\n",
        "                     hyp_dict['NUM_LAYERS'],\n",
        "                     hyp_dict['CONV_LAYER_SIZES'],\n",
        "                     hyp_dict['FILTER_SIZES'],\n",
        "                     hyp_dict['DROPOUT_RATES'],\n",
        "                     hyp_dict['FINAL_DENSE_LAYER_SIZE'])\n",
        "  print(model.summary())\n",
        "  trained_model = train_model(model,\n",
        "                              X_train,\n",
        "                              X_test,\n",
        "                              y_train_gender,\n",
        "                              y_dev_gender,\n",
        "                              hyp_dict['NUM_EPOCHS'],\n",
        "                              hyp_dict['BATCH_SIZE']\n",
        "                              )\n",
        "  \n",
        "  # adding epoch time just in case we accidentally re-use the same prefixes \n",
        "  gender_model_path = '{}_model_{}_gender_{}.h5'.format(MODEL_PREFIX, i, timestamp)\n",
        "  model.save(gender_model_path)\n",
        "  !gsutil cp /content/{gender_model_path} gs://fb-congressional-data/models/{gender_model_path}\n",
        "\n",
        "  preds = model.predict(X_test)\n",
        "  dev_df['probs'] = preds\n",
        "  dev_df['preds'] = dev_df.apply(pred_to_label, axis=1)\n",
        "\n",
        "  if 'W' in dev_df.preds.value_counts():\n",
        "    proportion_women_predicted = dev_df.preds.value_counts()['W'] / len(dev_df)\n",
        "  else:\n",
        "    proportion_women_predicted = 0\n",
        "  print(\"Proportion of predictions for W class: {}\".format(proportion_women_predicted))\n",
        "  plt.figure()\n",
        "  print(dev_df.probs.hist(bins=20))\n",
        "\n",
        "  # party model\n",
        "  model2 = make_model(embedding_matrix, \n",
        "                      hyp_dict['MAX_SEQUENCE_LENGTH'],\n",
        "                      hyp_dict['NUM_LAYERS'],\n",
        "                      hyp_dict['CONV_LAYER_SIZES'],\n",
        "                      hyp_dict['FILTER_SIZES'],\n",
        "                      hyp_dict['DROPOUT_RATES'],\n",
        "                      hyp_dict['FINAL_DENSE_LAYER_SIZE'])\n",
        "  trained_model2 = train_model(model2,\n",
        "                               X_train_party,\n",
        "                               X_test_party,\n",
        "                               y_train_party,\n",
        "                               y_dev_party,\n",
        "                               hyp_dict['NUM_EPOCHS'],\n",
        "                               hyp_dict['BATCH_SIZE'])\n",
        "  \n",
        "  party_model_path = '{}_model_{}_party_{}.h5'.format(MODEL_PREFIX, i, timestamp)\n",
        "  model2.save(party_model_path) # Note: changed this to model2\n",
        "  !gsutil cp /content/{party_model_path} gs://fb-congressional-data/models/{party_model_path}\n",
        "\n",
        "  preds2 = model2.predict(X_test)\n",
        "  dev_df['probs2'] = preds2\n",
        "  dev_df['preds2'] = dev_df.apply(pred_to_party_label, axis=1)\n",
        "\n",
        "  if 'Congress_Republican' in dev_df.preds2.value_counts():\n",
        "    proportion_republican_predicted = dev_df.preds2.value_counts()['Congress_Republican'] / len(dev_df)\n",
        "  else:\n",
        "    proportion_republican_predicted = 0\n",
        "  print(\"Proportion of predictions for Republican class: {}\".format(proportion_republican_predicted))\n",
        "  plt.figure()\n",
        "  print(dev_df.probs2.hist(bins=20))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Model 0\n",
            "Limiting by responses per post resulted in 3962284 training rows\n",
            "Removing responses under 3 words resulted in 10000 training rows\n",
            "Getting labels\n",
            "Getting inputs\n",
            "Tokenized in 00 minutes, 00 seconds\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 20, 50)            250150    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 20, 128)           19328     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 20, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 20, 64)            16448     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20, 64)            0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 286,587\n",
            "Trainable params: 36,437\n",
            "Non-trainable params: 250,150\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 10000 samples, validate on 2292907 samples\n",
            "10000/10000 [==============================] - 9s 881us/sample - loss: 0.0687 - acc: 0.9982 - val_loss: 1.0600 - val_acc: 0.8497\n",
            "Trained in 00 minutes, 09 seconds\n",
            "Copying file:///content/test2_model_0_gender_1574278190.4117863.h5 [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/1.4 MiB.                                      \n",
            "Proportion of predictions for W class: 0\n",
            "AxesSubplot(0.125,0.125;0.775x0.755)\n",
            "Train on 10000 samples, validate on 2292907 samples\n",
            "10000/10000 [==============================] - 6s 559us/sample - loss: 0.2920 - acc: 0.8918 - val_loss: 0.8552 - val_acc: 0.7407\n",
            "Trained in 00 minutes, 05 seconds\n",
            "Copying file:///content/test2_model_0_party_1574278190.4117863.h5 [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/1.4 MiB.                                      \n",
            "Training Model 1\n",
            "Limiting by responses per post resulted in 1754816 training rows\n",
            "Removing responses under 3 words resulted in 10000 training rows\n",
            "Getting labels\n",
            "Getting inputs\n",
            "Tokenized in 00 minutes, 00 seconds\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 20, 50)            250150    \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 20, 128)           19328     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 20, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 20, 64)            16448     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 20, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 20, 32)            6176      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 20, 32)            0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 292,443\n",
            "Trainable params: 42,293\n",
            "Non-trainable params: 250,150\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 10000 samples, validate on 2292907 samples\n",
            "10000/10000 [==============================] - 6s 601us/sample - loss: 0.2035 - acc: 0.9672 - val_loss: 0.8361 - val_acc: 0.8497\n",
            "Trained in 00 minutes, 06 seconds\n",
            "Copying file:///content/test2_model_1_gender_1574278190.4117863.h5 [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/1.5 MiB.                                      \n",
            "Proportion of predictions for W class: 0\n",
            "AxesSubplot(0.125,0.125;0.775x0.755)\n",
            "Train on 10000 samples, validate on 2292907 samples\n",
            "10000/10000 [==============================] - 6s 614us/sample - loss: 0.7663 - acc: 0.5296 - val_loss: 0.8450 - val_acc: 0.2593\n",
            "Trained in 00 minutes, 06 seconds\n",
            "Copying file:///content/test2_model_1_party_1574278190.4117863.h5 [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/1.5 MiB.                                      \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAD4CAYAAADCb7BPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niwEiSepigEiSuhggkqQu/w8N7tK4lFJtSAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAD4CAYAAADCb7BPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlv8Pxtk3HBTC7AUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aItuYkpo7N-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "69ca5d40-6386-4b20-ccb7-09617feb6631"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proportion of predictions for Republican class: 0\n",
            "AxesSubplot(0.125,0.125;0.775x0.755)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD7CAYAAACfQGjDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}