{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperparam_experiments_incl_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emrapport/w266-final-project/blob/master/LP_hyperparam_experiments_max_seq3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVfEbx0cZNNp",
        "colab_type": "text"
      },
      "source": [
        "Total 10 models; this notebook has 4 to ??. MAX_SEQUENCE_LENGTH: {10,20,30,50,100} and for each sample first words in one model and  half first, half last in a second model. Constant 2 layers, 32 layer size, 2 filter size, .2 dropout rate, drop 1-2 word responses, max 50 responses per post."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zHeoGlIX2sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# name of subfolder under models/ where trained models should go\n",
        "MODEL_PREFIX = \"LPFriday11_22\"\n",
        "\n",
        "hyp_combos = [{'NUM_EPOCHS': 10,\n",
        "                'BATCH_SIZE': 1000,\n",
        "                'MAX_SEQUENCE_LENGTH': 100,\n",
        "                'N_MOST_FREQ_WORDS_TO_KEEP': 5000,\n",
        "                'MAX_RESPONSES_PER_POST': 50,\n",
        "                'NUM_LAYERS': 2,\n",
        "                'CONV_LAYER_SIZES': [32,32],\n",
        "                'FILTER_SIZES': [2,2],\n",
        "                'DROPOUT_RATES': [.2,.2],\n",
        "                'FINAL_DENSE_LAYER_SIZE': 10,\n",
        "                # embed dim needs to map to one of the glove versions: 50, 100, 200, 300 \n",
        "                'EMBEDDING_DIM': 50,\n",
        "                'REMOVE_SHORT_RESP_LENGTH':0,\n",
        "                'SAMPLE_FROM_BEG_AND_END':False}\n",
        "                ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGfRSm8JdMvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## check to ensure hyps are in agreement\n",
        "for hyp_combo in hyp_combos:\n",
        "  assert hyp_combo['NUM_LAYERS'] == len(hyp_combo['CONV_LAYER_SIZES'])\n",
        "  assert hyp_combo['NUM_LAYERS'] == len(hyp_combo['FILTER_SIZES'])\n",
        "  assert hyp_combo['NUM_LAYERS'] == len(hyp_combo['DROPOUT_RATES'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s79t_aJZsYG",
        "colab_type": "code",
        "outputId": "4213912d-9ab0-4d4c-91f8-358106bc0251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "## all this stuff just needs to get run one time per notebook\n",
        "# Set seeds for reproducible results.\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from scipy.sparse import hstack, vstack\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import Sequential, layers\n",
        "from keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "import time\n",
        "import pickle\n",
        "!pip install gcsfs\n",
        "\n",
        "pd.set_option('max_colwidth', 100)\n",
        "\n",
        "project_id = 'w266-251323'\n",
        "import uuid\n",
        "bucket_name = 'fb-congressional-data/'\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcloud config set project {project_id}\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting gcsfs\n",
            "  Downloading https://files.pythonhosted.org/packages/65/e2/05f903ce8f77804127195cfcc1ca8b500a3157a1572dcc4b82bf5af01564/gcsfs-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.21.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.6.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.4.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.12.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n",
            "Installing collected packages: gcsfs\n",
            "Successfully installed gcsfs-0.4.0\n",
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OugAe-Vx04Pf",
        "colab_type": "code",
        "outputId": "08a814c5-f3b4-49e8-cfbd-55465085d891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "train_df = pd.read_csv(\"gs://fb-congressional-data/contraction_expanded_data/train.csv\", index_col=0)\n",
        "dev_df = pd.read_csv(\"gs://fb-congressional-data/contraction_expanded_data/dev.csv\", index_col=0)\n",
        "!gsutil cp gs://fb-congressional-data/glove* /tmp/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://fb-congressional-data/glove.6B.100d.txt...\n",
            "Copying gs://fb-congressional-data/glove.6B.200d.txt...\n",
            "Copying gs://fb-congressional-data/glove.6B.300d.txt...\n",
            "Copying gs://fb-congressional-data/glove.6B.50d.txt...\n",
            "- [4 files][  2.1 GiB/  2.1 GiB]   45.5 MiB/s                                   \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://fb-congressional-data/glove.6B.zip...\n",
            "- [5 files][  2.9 GiB/  2.9 GiB]   46.9 MiB/s                                   \n",
            "Operation completed over 5 objects/2.9 GiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysaWA5e_aGLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all the functions go here\n",
        "def remove_excess_rows_per_post(df, max_per_post):\n",
        "  num_responses_per_post = df.post_id.value_counts().reset_index()\n",
        "  num_responses_per_post.columns = ['post_id', 'num_responses']\n",
        "  \n",
        "  too_big_posts = num_responses_per_post[num_responses_per_post.num_responses > max_per_post]\n",
        "  posts_to_sample = too_big_posts.post_id.values\n",
        "  \n",
        "  # this gets all the rows for posts we DON'T need to sample \n",
        "  new_train_df = df[~df.post_id.isin(posts_to_sample)]\n",
        "  # this should be true\n",
        "  assert(len(too_big_posts) + new_train_df.post_id.nunique() == df.post_id.nunique())\n",
        "  \n",
        "  too_big_post_rows = df[df.post_id.isin(posts_to_sample)]\n",
        "  sampled_rows = too_big_post_rows.groupby('post_id').apply(lambda x: x.sample(n=max_per_post)).reset_index(drop=True)\n",
        "  new_train_df = pd.concat([new_train_df, sampled_rows])\n",
        "  \n",
        "  return new_train_df\n",
        "\n",
        "def get_labels(train_df, test_df, party_label_ind):\n",
        "\n",
        "  def turn_to_ints(li):\n",
        "    final_list = []\n",
        "    for gender in li:\n",
        "        if gender=='M':\n",
        "            final_list.append(1)\n",
        "        else:\n",
        "            final_list.append(0)\n",
        "    male = sum(final_list)\n",
        "    female = len(final_list)-sum(final_list)\n",
        "    percent_male = sum(final_list)/len(final_list)\n",
        "    print('total M: {}, total W: {}, percent M: {}'.format(male,female,percent_male))\n",
        "    return final_list\n",
        "\n",
        "  def turn_to_ints_party(li):\n",
        "    final_list = []\n",
        "    for party in li:\n",
        "        if party=='Congress_Republican':\n",
        "            final_list.append(1)\n",
        "        else:\n",
        "            final_list.append(0)\n",
        "    republican = sum(final_list)\n",
        "    not_repub = len(final_list)-sum(final_list)\n",
        "    percent_repub = sum(final_list)/len(final_list)\n",
        "    print('total republican: {}, total not republican: {}, percent republican: {}'.format(republican,not_repub,percent_repub))\n",
        "    return final_list\n",
        "\n",
        "  if party_label_ind:\n",
        "\n",
        "    y_train = train_df.op_category.values\n",
        "    y_dev = test_df.op_category.values\n",
        "    print('training set:')\n",
        "    y_train = turn_to_ints_party(y_train)\n",
        "    print('dev set:')\n",
        "    y_dev = turn_to_ints_party(y_dev) \n",
        "\n",
        "  else:\n",
        "    y_train = train_df.op_gender.values\n",
        "    y_dev = test_df.op_gender.values\n",
        "    print('training set:')\n",
        "    y_train = turn_to_ints(y_train)\n",
        "    print('dev set:')\n",
        "    y_dev = turn_to_ints(y_dev)\n",
        "\n",
        "  y_train = np.asarray(y_train)\n",
        "  y_dev = np.asarray(y_dev)\n",
        "\n",
        "  return y_train, y_dev\n",
        "\n",
        "def get_inputs(train_df, \n",
        "               test_df, \n",
        "               party_train_df,\n",
        "               party_dev_df,\n",
        "               n_words_to_keep,\n",
        "               max_seq_length):\n",
        "  def get_text_list(init_list):\n",
        "      sentences = []\n",
        "      for sentence in init_list:\n",
        "          if type(sentence) != str:\n",
        "              sentences.append(\"\")\n",
        "          else:\n",
        "              sentences.append(sentence)\n",
        "      return sentences\n",
        "\n",
        "  new_sentences_train = get_text_list(train_df.response_text.values)\n",
        "  new_sentences_test = get_text_list(dev_df.response_text.values)\n",
        "  party_new_sentences_train = get_text_list(party_train_df.response_text.values)\n",
        "  party_new_sentences_test = get_text_list(party_dev_df.response_text.values)\n",
        "\n",
        "  time_start = time.time()\n",
        "\n",
        "  # this is the default list of filters + apostrophe\n",
        "  # added because we have dealt with common contractions, so other apostrophes should mostly be possessive \n",
        "  tokenizer = Tokenizer(filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', oov_token='UNK')\n",
        "  tokenizer.fit_on_texts(new_sentences_train)\n",
        "\n",
        "\n",
        "  currentTime = time.gmtime(time.time() - time_start)\n",
        "\n",
        "  #Convert the gmtime struct to a string\n",
        "  timeStr = time.strftime(\"%M minutes, %S seconds\", currentTime)\n",
        "\n",
        "  print(\"Tokenized in {}\".format(timeStr))\n",
        "\n",
        "  # suggestion from this issue: https://github.com/keras-team/keras/issues/8092\n",
        "  # seems like OOV and num_words don't work correctly by default \n",
        "  tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() \n",
        "                              if i <= n_words_to_keep + 1} \n",
        "  tokenizer.word_index[tokenizer.oov_token] = n_words_to_keep + 1\n",
        "\n",
        "  X_train = tokenizer.texts_to_sequences(new_sentences_train)\n",
        "  X_test = tokenizer.texts_to_sequences(new_sentences_test)\n",
        "  X_train = pad_sequences(X_train, padding='post', maxlen=max_seq_length)\n",
        "  X_test = pad_sequences(X_test, padding='post', maxlen=max_seq_length)\n",
        "\n",
        "  X_train_party = tokenizer.texts_to_sequences(party_new_sentences_train)\n",
        "  X_test_party = tokenizer.texts_to_sequences(party_new_sentences_test)\n",
        "  X_train_party = pad_sequences(X_train_party, padding='post', maxlen=max_seq_length)\n",
        "  X_test_party = pad_sequences(X_test_party, padding='post', maxlen=max_seq_length)\n",
        "  return X_train, X_test, X_train_party, X_test_party, tokenizer\n",
        "\n",
        "def create_embedding_matrix(filepath, \n",
        "                            word_index, \n",
        "                            embedding_dim):\n",
        "    vocab_size = len(word_index) + 2  # Now we have to add 2 (reserved 0 plus the manual UNK token)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "def make_model(embedding_matrix, \n",
        "               max_seq_length,\n",
        "               num_layers,\n",
        "               conv_layer_size,\n",
        "               filter_size,\n",
        "               dropout_rate,\n",
        "               final_hidden_dense_size):\n",
        "  model = Sequential()\n",
        "\n",
        "  try:\n",
        "    model.add(layers.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], \n",
        "                              weights=[embedding_matrix], \n",
        "                              input_length=max_seq_length, \n",
        "                              trainable=False))\n",
        "    for i in range(num_layers):\n",
        "      model.add(layers.Conv1D(conv_layer_size[i], filter_size[i], activation='relu', padding=\"same\"))\n",
        "      model.add(layers.Dropout(dropout_rate[i]))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(final_hidden_dense_size, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "  except Exception as ex:\n",
        "    print(ex)\n",
        "  finally:\n",
        "    return model\n",
        "\n",
        "def train_model(model, \n",
        "                train_inputs,\n",
        "                dev_inputs,\n",
        "                train_labels,\n",
        "                dev_labels,\n",
        "                num_epochs,\n",
        "                batch_size):\n",
        "  try:\n",
        "    time_start = time.time()\n",
        "\n",
        "    history = model.fit(train_inputs, train_labels,\n",
        "                        epochs=num_epochs,\n",
        "                        verbose=True,\n",
        "                        validation_data=(dev_inputs, dev_labels),\n",
        "                        batch_size=batch_size)\n",
        "\n",
        "    currentTime = time.gmtime(time.time() - time_start)\n",
        "\n",
        "    #Convert the gmtime struct to a string\n",
        "    timeStr = time.strftime(\"%M minutes, %S seconds\", currentTime)\n",
        "\n",
        "    print(\"Trained in {}\".format(timeStr))\n",
        "\n",
        "  except Exception as ex:\n",
        "    print(\"Exception: {}\".format(ex))\n",
        "    currentTime = time.gmtime(time.time() - time_start)\n",
        "\n",
        "    #Convert the gmtime struct to a string\n",
        "    timeStr = time.strftime(\"%M minutes, %S seconds\", currentTime)\n",
        "\n",
        "    print(\"Trained in {}\".format(timeStr))  \n",
        "  finally:\n",
        "    return model\n",
        "\n",
        "def pred_to_label(row):\n",
        "  if row['probs'] >= .5:\n",
        "    return 'M'\n",
        "  else:\n",
        "    return 'W'\n",
        "\n",
        "def pred_to_party_label(row):\n",
        "  if row['probs2'] >= .5:\n",
        "    return 'Congress_Republican'\n",
        "  else:\n",
        "    return 'Not_Republican'\n",
        "\n",
        "def remove_short_responses(responses_df,n):\n",
        "  '''Takes a dataframe that includes a response_text column and removes \n",
        "  responses shorter than or equal to n. Returns new dataframe.'''\n",
        "  responses_df['split_response'] = responses_df['response_text'].str.split()\n",
        "  mask = (responses_df['split_response'].str.len() > n)\n",
        "  responses_df = responses_df.loc[mask]\n",
        "  responses_df = responses_df.drop(columns='split_response', axis = 1)\n",
        "  return responses_df\n",
        "\n",
        "def shorten_single_response(series_list,length):\n",
        "  '''Take a list of strings and a goal max length. Return the goal length list\n",
        "  taken half from the beginning of the orginal list and half from the end of \n",
        "  the original list.'''\n",
        "  if type(series_list) != float:\n",
        "    if len(series_list) > length:\n",
        "      new_list = series_list[:(length//2+length % 2)] + series_list[-length//2:]\n",
        "      return new_list\n",
        "    else: \n",
        "      return series_list\n",
        "  else:\n",
        "    return series_list\n",
        "\n",
        "def get_beg_end_responses(responses_df,n):\n",
        "  '''Takes a dataframe that includes a response_text column, creates new\n",
        "  response_text strings of word length n using the first n/2 words and last n/2\n",
        "  words of the response_text and returns a new dataframe with the new response_text.'''\n",
        "  responses_df['split_response'] = responses_df['response_text'].str.split()\n",
        "  responses_df['short_response'] = responses_df['split_response'].apply(shorten_single_response,args=(n,))\n",
        "  responses_df['response_text'] = responses_df['short_response'].str.join(' ')\n",
        "  responses_df = responses_df.drop(columns=['split_response','short_response'], axis=1)\n",
        "  return responses_df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s844qbPgZi2s",
        "colab_type": "code",
        "outputId": "f754cd4b-e4a3-4c03-85ba-cd2ffea24bcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# timestamp is shared across all runs\n",
        "timestamp = time.time()\n",
        "for i, hyp_dict in enumerate(hyp_combos):\n",
        "  print(\"Training Model {}\".format(i))\n",
        "  new_train_df = remove_excess_rows_per_post(train_df, hyp_dict['MAX_RESPONSES_PER_POST'])\n",
        "  print(\"Limiting to {} responses per post resulted in {} training rows\".format(hyp_dict['MAX_RESPONSES_PER_POST'],new_train_df.shape[0]))\n",
        "  if hyp_dict['REMOVE_SHORT_RESP_LENGTH'] > 0:\n",
        "    new_train_df = remove_short_responses(new_train_df,hyp_dict['REMOVE_SHORT_RESP_LENGTH'])\n",
        "    dev_df =remove_short_responses(dev_df,hyp_dict['REMOVE_SHORT_RESP_LENGTH'])\n",
        "    print(\"Removing responses under {} words resulted in {} training rows\".format((hyp_dict['REMOVE_SHORT_RESP_LENGTH']+1),new_train_df.shape[0]))\n",
        "    ######### Add if using test data###########\n",
        "    # test_df = remove_short_responses(test_df,hyp_dict['REMOVE_SHORT_RESP_LENGTH'])\n",
        "    ###########################################\n",
        "  if hyp_dict['SAMPLE_FROM_BEG_AND_END']:\n",
        "    length = hyp_dict['MAX_SEQUENCE_LENGTH']\n",
        "    new_train_df = get_beg_end_responses(new_train_df,length)\n",
        "    dev_df = get_beg_end_responses(dev_df,length)\n",
        "    ######### Add if using test data###########\n",
        "    # test_df = get_beg_end_responses(test_df,length)\n",
        "    ###########################################\n",
        "    print(\"Responses include only the first {} and last {} words\".format((length//2+length%2),length//2))\n",
        "  else:\n",
        "    print(\"Responses include only the first {} words\".format(hyp_dict['MAX_SEQUENCE_LENGTH']))\n",
        "\n",
        "  new_train_df = new_train_df.sample(frac=1)\n",
        "  dev_df = dev_df.sample(frac=1)\n",
        "\n",
        "  party_train_df = new_train_df[new_train_df.op_category!='Congress_Independent']\n",
        "  party_dev_df = dev_df[dev_df.op_category!='Congress_Independent']\n",
        "\n",
        "  # get two sets of labels: gender and party\n",
        "  print(\"Getting labels\")\n",
        "  y_train_gender, y_dev_gender = get_labels(new_train_df, dev_df, False)\n",
        "  y_train_party, y_dev_party = get_labels(party_train_df, party_dev_df, True)\n",
        "\n",
        "  print(\"Getting inputs\")\n",
        "  X_train, X_test, X_train_party, X_test_party, tokenizer = get_inputs(new_train_df, \n",
        "                                                                       dev_df, \n",
        "                                                                       party_train_df,\n",
        "                                                                       party_dev_df,\n",
        "                                                                       hyp_dict['N_MOST_FREQ_WORDS_TO_KEEP'], \n",
        "                                                                       hyp_dict['MAX_SEQUENCE_LENGTH'])\n",
        "  embedding_matrix = create_embedding_matrix(\n",
        "                     '/tmp/glove.6B.{}d.txt'.format(hyp_dict['EMBEDDING_DIM']),\n",
        "                      tokenizer.word_index, hyp_dict['EMBEDDING_DIM'])\n",
        "  \n",
        "  # gender model\n",
        "  model = make_model(embedding_matrix, \n",
        "                     hyp_dict['MAX_SEQUENCE_LENGTH'],\n",
        "                     hyp_dict['NUM_LAYERS'],\n",
        "                     hyp_dict['CONV_LAYER_SIZES'],\n",
        "                     hyp_dict['FILTER_SIZES'],\n",
        "                     hyp_dict['DROPOUT_RATES'],\n",
        "                     hyp_dict['FINAL_DENSE_LAYER_SIZE'])\n",
        "  print(model.summary())\n",
        "  trained_model = train_model(model,\n",
        "                              X_train,\n",
        "                              X_test,\n",
        "                              y_train_gender,\n",
        "                              y_dev_gender,\n",
        "                              hyp_dict['NUM_EPOCHS'],\n",
        "                              hyp_dict['BATCH_SIZE']\n",
        "                              )\n",
        "  \n",
        "  # adding epoch time just in case we accidentally re-use the same prefixes \n",
        "  gender_model_path = '{}_model_{}_gender_{}.h5'.format(MODEL_PREFIX, i, timestamp)\n",
        "  model.save(gender_model_path)\n",
        "  !gsutil cp /content/{gender_model_path} gs://fb-congressional-data/models/{gender_model_path}\n",
        "\n",
        "  preds = model.predict(X_test)\n",
        "  dev_df['probs'] = preds\n",
        "  dev_df['preds'] = dev_df.apply(pred_to_label, axis=1)\n",
        "\n",
        "  if 'W' in dev_df.preds.value_counts():\n",
        "    proportion_women_predicted = dev_df.preds.value_counts()['W'] / len(dev_df)\n",
        "  else:\n",
        "    proportion_women_predicted = 0\n",
        "  print(\"Proportion of predictions for W class: {}\".format(proportion_women_predicted))\n",
        "  plt.figure()\n",
        "  plt.title('Probablities for Gender Prediction, Model {}'.format(i))\n",
        "  dev_df.probs.hist(bins=20)\n",
        "  plt.show()\n",
        "\n",
        "  # party model\n",
        "  model2 = make_model(embedding_matrix, \n",
        "                      hyp_dict['MAX_SEQUENCE_LENGTH'],\n",
        "                      hyp_dict['NUM_LAYERS'],\n",
        "                      hyp_dict['CONV_LAYER_SIZES'],\n",
        "                      hyp_dict['FILTER_SIZES'],\n",
        "                      hyp_dict['DROPOUT_RATES'],\n",
        "                      hyp_dict['FINAL_DENSE_LAYER_SIZE'])\n",
        "  trained_model2 = train_model(model2,\n",
        "                               X_train_party,\n",
        "                               X_test_party,\n",
        "                               y_train_party,\n",
        "                               y_dev_party,\n",
        "                               hyp_dict['NUM_EPOCHS'],\n",
        "                               hyp_dict['BATCH_SIZE'])\n",
        "  \n",
        "  party_model_path = '{}_model_{}_party_{}.h5'.format(MODEL_PREFIX, i, timestamp)\n",
        "  model2.save(party_model_path) # Note: changed this to model2\n",
        "  !gsutil cp /content/{party_model_path} gs://fb-congressional-data/models/{party_model_path}\n",
        "\n",
        "  preds2 = model2.predict(X_test)\n",
        "  dev_df['probs2'] = preds2\n",
        "  dev_df['preds2'] = dev_df.apply(pred_to_party_label, axis=1)\n",
        "\n",
        "  if 'Congress_Republican' in dev_df.preds2.value_counts():\n",
        "    proportion_republican_predicted = dev_df.preds2.value_counts()['Congress_Republican'] / len(dev_df)\n",
        "  else:\n",
        "    proportion_republican_predicted = 0\n",
        "  print(\"Proportion of predictions for Republican class: {}\".format(proportion_republican_predicted))\n",
        "  plt.figure()\n",
        "  plt.title('Probablities for Party Prediction, Model {}'.format(i))\n",
        "  dev_df.probs2.hist(bins=20)\n",
        "  plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Model 0\n",
            "Limiting to 50 responses per post resulted in 3962284 training rows\n",
            "Responses include only the first 100 words\n",
            "Getting labels\n",
            "training set:\n",
            "total M: 2900753, total W: 1061531, percent M: 0.7320911373339215\n",
            "dev set:\n",
            "total M: 1948371, total W: 344536, percent M: 0.8497383452534272\n",
            "training set:\n",
            "total republican: 2459013, total not republican: 1486355, percent republican: 0.623265814494364\n",
            "dev set:\n",
            "total republican: 1698252, total not republican: 594655, percent republican: 0.7406545490070029\n",
            "Getting inputs\n",
            "Tokenized in 02 minutes, 25 seconds\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (None, 100, 50)           250150    \n",
            "_________________________________________________________________\n",
            "conv1d_28 (Conv1D)           (None, 100, 32)           3232      \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 100, 32)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_29 (Conv1D)           (None, 100, 32)           2080      \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 100, 32)           0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_14 (Glo (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 255,803\n",
            "Trainable params: 5,653\n",
            "Non-trainable params: 250,150\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 3962284 samples, validate on 2292907 samples\n",
            "Epoch 1/10\n",
            "3962284/3962284 [==============================] - 40s 10us/sample - loss: 0.5102 - acc: 0.7708 - val_loss: 0.4384 - val_acc: 0.8464\n",
            "Epoch 2/10\n",
            "3962284/3962284 [==============================] - 39s 10us/sample - loss: 0.4923 - acc: 0.7784 - val_loss: 0.4433 - val_acc: 0.8471\n",
            "Epoch 3/10\n",
            "3962284/3962284 [==============================] - 40s 10us/sample - loss: 0.4889 - acc: 0.7796 - val_loss: 0.4483 - val_acc: 0.8457\n",
            "Epoch 4/10\n",
            "3962284/3962284 [==============================] - 40s 10us/sample - loss: 0.4874 - acc: 0.7801 - val_loss: 0.4521 - val_acc: 0.8462\n",
            "Epoch 5/10\n",
            "3962284/3962284 [==============================] - 39s 10us/sample - loss: 0.4864 - acc: 0.7805 - val_loss: 0.4523 - val_acc: 0.8447\n",
            "Epoch 6/10\n",
            "3962284/3962284 [==============================] - 39s 10us/sample - loss: 0.4857 - acc: 0.7808 - val_loss: 0.4423 - val_acc: 0.8479\n",
            "Epoch 7/10\n",
            "3962284/3962284 [==============================] - 39s 10us/sample - loss: 0.4852 - acc: 0.7810 - val_loss: 0.4496 - val_acc: 0.8476\n",
            "Epoch 8/10\n",
            "3962284/3962284 [==============================] - 39s 10us/sample - loss: 0.4847 - acc: 0.7811 - val_loss: 0.4430 - val_acc: 0.8483\n",
            "Epoch 9/10\n",
            "3962284/3962284 [==============================] - 39s 10us/sample - loss: 0.4844 - acc: 0.7812 - val_loss: 0.4397 - val_acc: 0.8490\n",
            "Epoch 10/10\n",
            "3962284/3962284 [==============================] - 39s 10us/sample - loss: 0.4841 - acc: 0.7814 - val_loss: 0.4364 - val_acc: 0.8496\n",
            "Trained in 06 minutes, 34 seconds\n",
            "Copying file:///content/LPFriday11_22_model_0_gender_1574470644.9169657.h5 [Content-Type=application/octet-stream]...\n",
            "/ [1 files][  1.1 MiB/  1.1 MiB]                                                \n",
            "Operation completed over 1 objects/1.1 MiB.                                      \n",
            "Proportion of predictions for W class: 0.020466159333980837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEICAYAAACqMQjAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYVl3Bu9JfOcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 3945368 samples, validate on 2292907 samples\n",
            "Epoch 1/10\n",
            "3945368/3945368 [==============================] - 40s 10us/sample - loss: 0.6001 - acc: 0.6682 - val_loss: 0.5866 - val_acc: 0.7322\n",
            "Epoch 2/10\n",
            "3945368/3945368 [==============================] - 39s 10us/sample - loss: 0.5728 - acc: 0.6873 - val_loss: 0.5857 - val_acc: 0.7343\n",
            "Epoch 3/10\n",
            "3945368/3945368 [==============================] - 39s 10us/sample - loss: 0.5675 - acc: 0.6906 - val_loss: 0.5757 - val_acc: 0.7404\n",
            "Epoch 4/10\n",
            "3945368/3945368 [==============================] - 39s 10us/sample - loss: 0.5647 - acc: 0.6922 - val_loss: 0.5701 - val_acc: 0.7435\n",
            "Epoch 5/10\n",
            "3945368/3945368 [==============================] - 39s 10us/sample - loss: 0.5631 - acc: 0.6931 - val_loss: 0.5681 - val_acc: 0.7439\n",
            "Epoch 6/10\n",
            "3945368/3945368 [==============================] - 39s 10us/sample - loss: 0.5619 - acc: 0.6938 - val_loss: 0.5711 - val_acc: 0.7398\n",
            "Epoch 7/10\n",
            "3945368/3945368 [==============================] - 39s 10us/sample - loss: 0.5609 - acc: 0.6942 - val_loss: 0.5690 - val_acc: 0.7438\n",
            "Epoch 8/10\n",
            "3945368/3945368 [==============================] - 39s 10us/sample - loss: 0.5602 - acc: 0.6949 - val_loss: 0.5813 - val_acc: 0.7254\n",
            "Epoch 9/10\n",
            "3945368/3945368 [==============================] - 39s 10us/sample - loss: 0.5593 - acc: 0.6956 - val_loss: 0.5687 - val_acc: 0.7408\n",
            "Epoch 10/10\n",
            "3945368/3945368 [==============================] - 39s 10us/sample - loss: 0.5588 - acc: 0.6957 - val_loss: 0.5791 - val_acc: 0.7347\n",
            "Trained in 06 minutes, 29 seconds\n",
            "Copying file:///content/LPFriday11_22_model_0_party_1574470644.9169657.h5 [Content-Type=application/octet-stream]...\n",
            "/ [1 files][  1.1 MiB/  1.1 MiB]                                                \n",
            "Operation completed over 1 objects/1.1 MiB.                                      \n",
            "Proportion of predictions for Republican class: 0.8460055292255639\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEICAYAAACqMQjAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3df5xcdX3v8deb8CvlV4DgNiSRYIle\nA6kB9kJa27qFGpZoDe0FDUVIMBJboWrN7TXY3guCtPi4RSqtYqOkSfBHSFGbXA2mKTAPqm2AIEgI\naFlDMAkQJAmBlQIufu4f57twMszZmZ2Znc1k38/HYx575nO+33O+3zNn5zPne87MUURgZmZWyX7D\n3QAzM9t7OUmYmVkhJwkzMyvkJGFmZoWcJMzMrJCThJmZFXKS2AdJulLSV+qsO1fS9waYX5L0wTR9\ngaR/GaDsb0v6cT3tqNLGP5G0XVKvpKObvfx92WBevyrLuU3SnOa2rjUkTZIUkvavoeyA/w8jgZPE\nXkLSZkn/ld74tktaIunQ4W7XQCLiqxExo/95+sc7ITf/3yLiLc1cp6QDgM8CMyLi0IjY0YRlNm3b\nS+qStLWBtlwp6RepLc9K+ndJv1Hv8gZS/vpVadMeHzoi4uyIWDoU7Spb92ZJL0saWxa/P+1vk4a6\nDQORNE3SfZJeSH+nDWd7hoKTxN7l9yPiUOAUoBP4y/ICyozk160DOBjYONiKVbZd1W1fw/KrfjKt\n0S2pLccA3wO+KUlDuL693WPA+f1PJE0FfmX4mvNqOw4EVgJfAY4ElgIrU3yfMZLfbPZaEbENuA04\nCV4dIrhG0veBF4A3STpW0ipJOyX1SLqkbDEHS7pF0vOSfiDpbf0zJC2U9JM072FJf1BWV5L+XtJu\nST+SdGalduYPxSXdlcI/TJ+C31f+qTq1+RuSfibpMUkfyc07TdJ6Sc+lT/OfrbC+NwP9w1fPSroj\nxX9T0r2pvfdK+s1cnddtu8INT8Vtf7GkR9K22iTpQ7lld0naKukTkp4Cvp7qHpu2QW/q8wv5YTFJ\np6RtcECVtvyC7I3nV4Gj0/b+vqTrJe0ArkzL+0Bq4y5JayQdl1vXO9NruFvS3wPKzdtjKEXSiZLW\npn1qu6RPSuoGPgm8L/Xnh7nt2j9stZ+kv5T0uKSnJS2TdESa1z+0M0fSTyU9I+kvBup3BTcDF+We\nzwGW5QtIOiKt92epHX/Z/4FA0ihJf5PWvQl4V4W6N0l6UtI2SZ+WNKqGdnUB+wN/GxEvRcQNZNv3\njEH2b6/mJLEXkjQRmAncnwtfCMwHDgMeB5YDW4FjgXOBv5KU3zlnAf8EHAV8Dfjn3JvST4DfBo4A\nPgV8RdK4XN3TU5mxwBVkn2SPGqjNEfE7afJtaRjolrI+7Qf8P+CHwHjgTOBjks5KRT4HfC4iDgd+\nDVhRYR3/CZyYno6JiDNSu74D3AAcTTYU9R3tea6ifNsVqrDtnwbeDRwOXAxcL+mUXJVfJdvGx5G9\nkZ0NPJG2waER8QRQAt5b1p7lKQkM1JaDgLnAloh4JoVPBzaRHVFdI2kW2Zv4H5IdefwbWbJC2RDN\nN8mOisaSvaZvL1jXYcC/At8l26dOAG6PiO8Cf0U6uomIt1WoPjc9fpcsCR8K/H1Zmd8C3kL2uv8f\nSW8dqO9l1gGHS3prevOeTfbpPe/vyPbnNwHvIHstLk7zLiF7DU8mO0o8t6zuEqCPrM8nAzOAD9bQ\nrhOBB2PP3zZ6kNf20X1DRPixFzyAzUAv8CzZG9kXgNFpXgm4Kld2IvAKcFgu9tfAkjR9JbAuN28/\n4EngtwvW/QAwK03PBZ4AlJt/D3Bhri0fzJX9Xq5cACfknncBW9P06cBPy9Z7OfCPafousoQ1tsp2\nmpTWs396fiFwT1mZ/wDmVtp2g932Fcr+M/DRXP9eBg6u1Odc7H3A99P0KOAp4LSC5V+ZlvksWYK6\nAzg1t73Lt+FtwLyy1/oFXkta+f1AZB8sXvf6kQ3n3D9Am75SFsvvB7cDH87NewvwC7JP2f2v14Sy\n/Wn2IP4vfo8s0f010A2sTcuOtPxRaZtNydX7EFBK03cAf5ybN6N/HyJLti/lX++0Le6stI+Xte1/\nkyX7fOyrwJWNvBfsbY+RMqbZLs6JiH8tmLclN30ssDMins/FHif7lPS68hHxyzTscyyApIuAj5P9\ng0H2yS9/YnBbpD0+t+xjB9GPSo4jG4Z5NhcbRfbJF2AecBXwI0mPAZ+KiG/XsNxjef3RweNkRyv9\ntlBdxW0v6Wyyo6k3k70B/wqwIVfkZxHxYpVlrwS+KOl4sjfQ3RFxzwDlV0TE+wvmlfflOOBzkq7L\nN5us/8ey534Qkoq2xUSyI416lL8Gj/PaG3C/p3LTL5Dtc4NxM9kHieMpG2oi23cPqNCG/n1gj+1Q\nVu64VPdJvXbaZz9q22d6yY4w8w4Hnq9Qtm15uKl95N+0nwCOSkME/d4IbMs9n9g/kYZ6JgBPpPHq\nLwGXAUdHxBjgIXJj1cB4aY8TpW9M62zEFuCxiBiTexwWETMBIuLRiDgfeAPwGeBWSYfUsNwnyP7R\n88q3RV0/dZyGe74B/A3QkbbVavbcVuXLft26UhJZAbyf7Mjn5nraU7D8LcCHyrbr6Ij4d7Kjx/x+\noPzzCsspOl9TbfuVvwZvJBu+2V6lXs0i4nGyE9gzyYbQ8p4hO3Ipb0P/PrDHdkjz+m0hO5IYm9t+\nh0dELUNGG4FfL/tf+XXquKhib+Yk0YYiYgvw78BfSzpY0q+TfRLPj9OeKukPlV0B8zGyf4R1wCFk\n//Q/g+zELOkkbc4bgI9IOkDSecBbyd4cq9lO8RvNPcDz6STv6HQy8SRJ/z214/2SjomIX5INtQD8\nsoZ1rgbeLOmPJO0v6X3AFKCWo5BqDgQOIttWfemootolo9vJTjIfURZfRjZ08R4aSxLlvghcLulE\nePUk7Hlp3neAE3P7wUfIzqFU8m1gnKSPSTpI0mGSTk/ztgOTVHxl2NeBP5N0vLJLh/vPYfRVa7yy\nk/+1JvF5wBkR8fN8MCJeIUvC16R2H0d2pNz//7CCbH+eIOlIYGGu7pPAvwDXSTo8nYT/NUnvqKE9\nJbJh34+kbXZZit9RY3/agpNE+zqfbLjoCeBbwBVlwyUrycbCd5F9ev3DiPhFRDwMXEc2br8dmAp8\nv2zZdwOTyT6hXQOcG7V9H+FKYKmy6/vzJ2r7/5HfDUwj+0T4DPBlspONkI01b5TUS3YSe3ZE/Fe1\nFaZ2vRtYAOwA/hfw7njtRG/d0nDeR8jeZHYBfwSsqlLnR2RvmpvSdjg2xb9PlvR+kD4VN0VEfIvs\nyGu5pOfIjgrPTvOeAc4DriXbNpN5/Wvdv5zngXcCv082NPQo2YloyC6AANgh6QcVqi/mteGgx4AX\ngT+tsQsTyT7wVBURP4mI9QWz/xT4OdlJ/e+RXayxOM37ErCG7KKJH/D6I5GLyD4QPEz2Ot8KjKOK\niHgZOCfVfxb4ANmw5cu19KddaM+hZzMbKsou2f1aRHx5uNuyt5D0ZeCfImLNcLfFKnOSMGuBNKy2\nFphYdsGB2V7Nw01mQ0zSUrLvIHzMCcLajY8kzMyskI8kzMys0D73ZbqxY8fGpEmT6qr785//nEMO\nqeXS/H2H+zwyuM8jQyN9vu+++56JiGPK4/tckpg0aRLr1xddJTewUqlEV1dXcxu0l3OfRwb3eWRo\npM+SKl6a7eEmMzMr5CRhZmaFnCTMzKyQk4SZmRVykjAzs0JOEmZmVshJwszMCjlJmJlZIScJMzMr\nVPM3riWNAtaT3f/43el+vcuBo4H7gAsj4uV0y8dlwKlkNzp5X0RsTsu4nOzuUq8AH+n/DXlJ3WQ3\nmhkFfDkirk3xiutouNdmbWjSwu/UXXdJ98j6eQprnsEcSXwUeCT3/DPA9RFxAtndnOal+DxgV4pf\nn8ohaQowGziR7C5kX0i3sBwFfJ7sblpTgPNT2YHWYWZmLVBTkpA0AXgX2e0m+2+ofgbZbf4AlpLd\nxg9gVnpOmn9mKj8LWB4RL0XEY0APcFp69ETEpnSUsByYVWUdZmbWArUON/0t2b2DD0vPjwaezd3o\nfCswPk2PB7YARESfpN2p/HhgXW6Z+TpbyuKnV1nHHiTNB+YDdHR0UCqVauzWnnp7e+uu267c5/ax\nYGpf9UIF2rXPjXCfm6NqkpD0buDpiLhPUldT194kEbEIWATQ2dkZ9f4Kon81cmRo1z7PbfCcRDv2\nuRHt+jo3Yij6XMuRxNuB90iaCRwMHE52knmMpP3TJ/0JwLZUfhswEdgqaX/gCLIT2P3xfvk6leI7\nBliHmZm1QNVzEhFxeURMiIhJZCee74iIC4A7gXNTsTnAyjS9Kj0nzb8jsnukrgJmSzooXbU0GbgH\nuBeYLOl4SQemdaxKdYrWYWZmLdDI9yQ+AXxcUg/Z+YObUvwm4OgU/ziwECAiNgIrgIeB7wKXRsQr\n6SjhMmAN2dVTK1LZgdZhZmYtMKg700VECSil6U1kVyaVl3kROK+g/jXANRXiq4HVFeIV12FmZq3h\nb1ybmVmhfe4e12b2ehu27a776qjN176rya2xduIjCTMzK+QkYWZmhZwkzMyskJOEmZkVcpIwM7NC\nThJmZlbIScLMzAo5SZiZWSEnCTMzK+QkYWZmhZwkzMyskJOEmZkVcpIwM7NCThJmZlaoapKQdLCk\neyT9UNJGSZ9K8SWSHpP0QHpMS3FJukFSj6QHJZ2SW9YcSY+mx5xc/FRJG1KdGyQpxY+StDaVXyvp\nyOZvAjMzK1LLkcRLwBkR8TZgGtAtaXqa9+cRMS09Hkixs8nuXz0ZmA/cCNkbPnAFcDrZ3eauyL3p\n3whckqvXneILgdsjYjJwe3puZmYtUjVJRKY3PT0gPWKAKrOAZaneOmCMpHHAWcDaiNgZEbuAtWQJ\nZxxweESsi4gAlgHn5Ja1NE0vzcXNzKwFajonIWmUpAeAp8ne6O9Os65JQ0rXSzooxcYDW3LVt6bY\nQPGtFeIAHRHxZJp+CuiorVtmZtYMNd2+NCJeAaZJGgN8S9JJwOVkb9wHAouATwBXDVVDIyIkVTyC\nkTSfbGiLjo4OSqVSXevo7e2tu267cp/bx4KpfXXX7Rhdf/123FbQvq9zI4aiz4O6x3VEPCvpTqA7\nIv4mhV+S9I/A/0zPtwETc9UmpNg2oKssXkrxCRXKA2yXNC4inkzDUk8XtGsRWaKis7Mzurq6KhWr\nqlQqUW/dduU+t49671ENWYK4bkN9t7TffEFX3esdTu36OjdiKPpcy9VNx6QjCCSNBt4J/Ci9aZOu\nRDoHeChVWQVclK5ymg7sTkNGa4AZko5MJ6xnAGvSvOckTU/LughYmVtW/1VQc3JxMzNrgVo+WowD\nlkoaRZZUVkTEtyXdIekYQMADwB+n8quBmUAP8AJwMUBE7JR0NXBvKndVROxM0x8GlgCjgdvSA+Ba\nYIWkecDjwHvr7aiZmQ1e1SQREQ8CJ1eIn1FQPoBLC+YtBhZXiK8HTqoQ3wGcWa2NZmY2NPyNazMz\nK+QkYWZmhZwkzMyskJOEmZkVcpIwM7NCThJmZlbIScLMzAo5SZiZWSEnCTMzK1TfL36Z2YgxqYEf\nFtx87bua2BIbDj6SMDOzQk4SZmZWyEnCzMwKOUmYmVkhJwkzMyvkJGFmZoWcJMzMrFAt97g+WNI9\nkn4oaaOkT6X48ZLultQj6RZJB6b4Qel5T5o/Kbesy1P8x5LOysW7U6xH0sJcvOI6zMysNWo5kngJ\nOCMi3gZMA7olTQc+A1wfEScAu4B5qfw8YFeKX5/KIWkKMBs4EegGviBpVLp39ueBs4EpwPmpLAOs\nw8zMWqBqkohMb3p6QHoEcAZwa4ovBc5J07PSc9L8MyUpxZdHxEsR8RjQA5yWHj0RsSkiXgaWA7NS\nnaJ1mJlZC9T0sxzp0/59wAlkn/p/AjwbEX2pyFZgfJoeD2wBiIg+SbuBo1N8XW6x+TpbyuKnpzpF\n6yhv33xgPkBHRwelUqmWbr1Ob29v3XXblfvcPhZM7ateqEDH6Mbq12s4t3O7vs6NGIo+15QkIuIV\nYJqkMcC3gP/W1FY0KCIWAYsAOjs7o6urq67llEol6q3brtzn9jG3gd9QWjC1j+s2tP6n2jZf0NXy\ndfZr19e5EUPR50Fd3RQRzwJ3Ar8BjJHUv9dNALal6W3ARIA0/whgRz5eVqcovmOAdZiZWQvUcnXT\nMekIAkmjgXcCj5Ali3NTsTnAyjS9Kj0nzb8jIiLFZ6ern44HJgP3APcCk9OVTAeSndxeleoUrcPM\nzFqgluPPccDSdF5iP2BFRHxb0sPAckmfBu4HbkrlbwJultQD7CR70yciNkpaATwM9AGXpmEsJF0G\nrAFGAYsjYmNa1icK1mFmZi1QNUlExIPAyRXim8iuTCqPvwicV7Csa4BrKsRXA6trXYeZmbWGbzpk\n1iKN3LzHbLj4ZznMzKyQk4SZmRVykjAzs0JOEmZmVshJwszMCjlJmJlZIScJMzMr5CRhZmaFnCTM\nzKyQk4SZmRVykjAzs0JOEmZmVshJwszMCjlJmJlZIScJMzMrVMvtSydKulPSw5I2Svpoil8paZuk\nB9JjZq7O5ZJ6JP1Y0lm5eHeK9UhamIsfL+nuFL8l3caUdKvTW1L8bkmTmtl5MzMbWC1HEn3AgoiY\nAkwHLpU0Jc27PiKmpcdqgDRvNnAi0A18QdKodPvTzwNnA1OA83PL+Uxa1gnALmBeis8DdqX49amc\nmZm1SNUkERFPRsQP0vTzwCPA+AGqzAKWR8RLEfEY0EN2C9LTgJ6I2BQRLwPLgVmSBJwB3JrqLwXO\nyS1raZq+FTgzlTczsxYY1O1L03DPycDdwNuByyRdBKwnO9rYRZZA1uWqbeW1pLKlLH46cDTwbET0\nVSg/vr9ORPRJ2p3KP1PWrvnAfICOjg5KpdJguvWq3t7euuu2K/e5dRZM7ateaIh0jB6e9Q/nvuV9\nuzlqThKSDgW+AXwsIp6TdCNwNRDp73XAB5rauhpFxCJgEUBnZ2d0dXXVtZxSqUS9dduV+9w6c4fx\nHtcLpvZx3YbW39J+8wVdLV9nP+/bzVHT1U2SDiBLEF+NiG8CRMT2iHglIn4JfIlsOAlgGzAxV31C\nihXFdwBjJO1fFt9jWWn+Eam8mZm1QC1XNwm4CXgkIj6bi4/LFfsD4KE0vQqYna5MOh6YDNwD3AtM\nTlcyHUh2cntVRARwJ3Buqj8HWJlb1pw0fS5wRypvZmYtUMvx59uBC4ENkh5IsU+SXZ00jWy4aTPw\nIYCI2ChpBfAw2ZVRl0bEKwCSLgPWAKOAxRGxMS3vE8BySZ8G7idLSqS/N0vqAXaSJRYzM2uRqkki\nIr4HVLqiaPUAda4BrqkQX12pXkRs4rXhqnz8ReC8am00M7Oh4W9cm5lZIScJMzMr5CRhZmaFnCTM\nzKyQk4SZmRVykjAzs0JOEmZmVqj1P+ZiZiPGpAZ+r2rzte9qYkusXj6SMDOzQk4SZmZWyEnCzMwK\nOUmYmVkhJwkzMyvkJGFmZoWcJMzMrJCThJmZFXKSMDOzQrXc43qipDslPSxpo6SPpvhRktZKejT9\nPTLFJekGST2SHpR0Sm5Zc1L5RyXNycVPlbQh1bkh3Ve7cB1mZtYatRxJ9AELImIKMB24VNIUYCFw\ne0RMBm5PzwHOBianx3zgRsje8IErgNPJblV6Re5N/0bgkly97hQvWoeZmbVA1SQREU9GxA/S9PPA\nI8B4YBawNBVbCpyTpmcByyKzDhgjaRxwFrA2InZGxC5gLdCd5h0eEesiIoBlZcuqtA4zM2uBQf3A\nn6RJwMnA3UBHRDyZZj0FdKTp8cCWXLWtKTZQfGuFOAOso7xd88mOWujo6KBUKg2mW6/q7e2tu267\ncp9bZ8HUvpavs1/H6OFdfz0afY28bzdHzUlC0qHAN4CPRcRz6bQBABERkqKpLSsz0DoiYhGwCKCz\nszO6urrqWkepVKLeuu3KfW6duQ38ImqjFkzt47oN7fWjz5sv6Gqovvft5qjp6iZJB5AliK9GxDdT\neHsaKiL9fTrFtwETc9UnpNhA8QkV4gOtw8zMWqCWq5sE3AQ8EhGfzc1aBfRfoTQHWJmLX5SucpoO\n7E5DRmuAGZKOTCesZwBr0rznJE1P67qobFmV1mFmZi1Qy/Hn24ELgQ2SHkixTwLXAiskzQMeB96b\n5q0GZgI9wAvAxQARsVPS1cC9qdxVEbEzTX8YWAKMBm5LDwZYh5mZtUDVJBER3wNUMPvMCuUDuLRg\nWYuBxRXi64GTKsR3VFqHmZm1hr9xbWZmhZwkzMyskJOEmZkVcpIwM7NCThJmZlbIScLMzAo5SZiZ\nWSEnCTMzK+QkYWZmhZwkzMyskJOEmZkVaq8fmDcbZpOG8Z4QZsPBRxJmZlbIScLMzAo5SZiZWSEn\nCTMzK1TL7UsXS3pa0kO52JWStkl6ID1m5uZdLqlH0o8lnZWLd6dYj6SFufjxku5O8VskHZjiB6Xn\nPWn+pGZ12szMalPLkcQSoLtC/PqImJYeqwEkTQFmAyemOl+QNErSKODzwNnAFOD8VBbgM2lZJwC7\ngHkpPg/YleLXp3JmZtZCVZNERNwF7KxWLpkFLI+IlyLiMbL7XJ+WHj0RsSkiXgaWA7MkCTgDuDXV\nXwqck1vW0jR9K3BmKm9mZi3SyPckLpN0EbAeWBARu4DxwLpcma0pBrClLH46cDTwbET0VSg/vr9O\nRPRJ2p3KP1PeEEnzgfkAHR0dlEqlujrU29tbd9125T4PzoKpfdUL7YU6Rrdf2xvdL71vN0e9SeJG\n4Gog0t/rgA80q1GDFRGLgEUAnZ2d0dXVVddySqUS9dZtV+7z4Mxt0y/TLZjax3Ub2uu7s5sv6Gqo\nvvft5qjr6qaI2B4Rr0TEL4EvkQ0nAWwDJuaKTkixovgOYIyk/cvieywrzT8ilTczsxapK0lIGpd7\n+gdA/5VPq4DZ6cqk44HJwD3AvcDkdCXTgWQnt1dFRAB3Auem+nOAlbllzUnT5wJ3pPJmZtYiVY8/\nJX0d6ALGStoKXAF0SZpGNty0GfgQQERslLQCeBjoAy6NiFfSci4D1gCjgMURsTGt4hPAckmfBu4H\nbkrxm4CbJfWQnTif3XBvzcxsUKomiYg4v0L4pgqx/vLXANdUiK8GVleIb+K14ap8/EXgvGrtM7N9\nU6M/prik+5AmtWRk8zeuzcyskJOEmZkVcpIwM7NCThJmZlbIScLMzAo5SZiZWSEnCTMzK+QkYWZm\nhZwkzMyskJOEmZkVcpIwM7NCThJmZlbIScLMzAo5SZiZWSEnCTMzK+QkYWZmhZwkzMysUNUkIWmx\npKclPZSLHSVpraRH098jU1ySbpDUI+lBSafk6sxJ5R+VNCcXP1XShlTnBkkaaB1mZtY6tRxJLAG6\ny2ILgdsjYjJwe3oOcDYwOT3mAzdC9oZPdm/s08luVXpF7k3/RuCSXL3uKuswM7MWqZokIuIuYGdZ\neBawNE0vBc7JxZdFZh0wRtI44CxgbUTsjIhdwFqgO807PCLWRUQAy8qWVWkdZmbWIvvXWa8jIp5M\n008BHWl6PLAlV25rig0U31ohPtA6XkfSfLIjFzo6OiiVSoPsTqa3t7fuuu3KfR6cBVP7mtuYFukY\n3b5tr5f37eaoN0m8KiJCUjSjMfWuIyIWAYsAOjs7o6urq671lEol6q3brtznwZm78DvNbUyLLJja\nx3UbGv53bytLug/xvt0E9V7dtD0NFZH+Pp3i24CJuXITUmyg+IQK8YHWYWZmLVJvklgF9F+hNAdY\nmYtflK5ymg7sTkNGa4AZko5MJ6xnAGvSvOckTU9XNV1UtqxK6zAzsxapevwp6etAFzBW0layq5Su\nBVZImgc8Drw3FV8NzAR6gBeAiwEiYqekq4F7U7mrIqL/ZPiHya6gGg3clh4MsA4zM2uRqkkiIs4v\nmHVmhbIBXFqwnMXA4grx9cBJFeI7Kq3DzMxax9+4NjOzQk4SZmZWaGRdE2dmI8aGbbvrvmR587Xv\nanJr2pePJMzMrJCThJmZFXKSMDOzQk4SZmZWyCeubcRp5ISm2UjjIwkzMyvkJGFmZoWcJMzMrJCT\nhJmZFXKSMDOzQk4SZmZWyEnCzMwKOUmYmVmhhpKEpM2SNkh6QNL6FDtK0lpJj6a/R6a4JN0gqUfS\ng5JOyS1nTir/qKQ5ufipafk9qa4aaa+ZmQ1OM44kfjcipkVEZ3q+ELg9IiYDt6fnAGcDk9NjPnAj\nZEmF7JaopwOnAVf0J5ZU5pJcve4mtNfMzGo0FMNNs4ClaXopcE4uviwy64AxksYBZwFrI2JnROwC\n1gLdad7hEbEu3RZ1WW5ZZmbWAo3+dlMA/yIpgH+IiEVAR0Q8meY/BXSk6fHAllzdrSk2UHxrhfjr\nSJpPdnRCR0cHpVKprs709vbWXbddjcQ+d4yGBVP7hrsZLeU+D067/k8Mxf9zo0nityJim6Q3AGsl\n/Sg/MyIiJZAhlZLTIoDOzs7o6uqqazmlUol667arkdjnv/vqSq7bMLJ+23LB1D73eRA2X9DV3Ma0\nyFD8Pzc03BQR29Lfp4FvkZ1T2J6Gikh/n07FtwETc9UnpNhA8QkV4mZm1iJ1f7SQdAiwX0Q8n6Zn\nAFcBq4A5wLXp78pUZRVwmaTlZCepd0fEk5LWAH+VO1k9A7g8InZKek7SdOBu4CLg7+ptr5lZrSY1\n8FPy+9r9sRs5/uwAvpWuSt0f+FpEfFfSvcAKSfOAx4H3pvKrgZlAD/ACcDFASgZXA/emcldFxM40\n/WFgCTAauC09zMysRepOEhGxCXhbhfgO4MwK8QAuLVjWYmBxhfh64KR622hmZo0ZWWeybJ/RyHDA\ngqlNbIjZPs4/y2FmZoWcJMzMrJCThJmZFXKSMDOzQk4SZmZWyEnCzMwKOUmYmVkhf0/ChkUj33Mw\n25vtaz/p4SMJMzMr5CMJM7O9RKNH2Eu6D2lSS17jIwkzMyvkIwmrm88rmO37fCRhZmaFnCTMzKyQ\nh5tGuA3bdjPXw0ZmVsBJoknadXze91Yws4Hs9cNNkrol/VhSj6SFw90eM7ORZK8+kpA0Cvg88E5g\nK3CvpFUR8fBQrM9DL2Zme1YyCWIAAAQNSURBVNrbjyROA3oiYlNEvAwsB2YNc5vMzEYMRcRwt6GQ\npHOB7oj4YHp+IXB6RFxWVm4+MD89fQvw4zpXORZ4ps667cp9Hhnc55GhkT4fFxHHlAf36uGmWkXE\nImBRo8uRtD4iOpvQpLbhPo8M7vPIMBR93tuHm7YBE3PPJ6SYmZm1wN6eJO4FJks6XtKBwGxg1TC3\nycxsxNirh5siok/SZcAaYBSwOCI2DuEqGx6yakPu88jgPo8MTe/zXn3i2szMhtfePtxkZmbDyEnC\nzMwKjcgkUe2nPiQdJOmWNP9uSZNa38rmqqHPH5f0sKQHJd0u6bjhaGcz1fqTLpL+h6SQ1PaXS9bS\nZ0nvTa/1Rklfa3Ubm62GffuNku6UdH/av2cORzubRdJiSU9LeqhgviTdkLbHg5JOaWiFETGiHmQn\nwH8CvAk4EPghMKWszIeBL6bp2cAtw93uFvT5d4FfSdN/MhL6nModBtwFrAM6h7vdLXidJwP3A0em\n528Y7na3oM+LgD9J01OAzcPd7gb7/DvAKcBDBfNnArcBAqYDdzeyvpF4JFHLT33MApam6VuBMyWp\nhW1stqp9jog7I+KF9HQd2XdS2lmtP+lyNfAZ4MVWNm6I1NLnS4DPR8QugIh4usVtbLZa+hzA4Wn6\nCOCJFrav6SLiLmDnAEVmAcsisw4YI2lcvesbiUliPLAl93xrilUsExF9wG7g6Ja0bmjU0ue8eWSf\nRNpZ1T6nw/CJEbGv/KpjLa/zm4E3S/q+pHWSulvWuqFRS5+vBN4vaSuwGvjT1jRt2Az2/31Ae/X3\nJKz1JL0f6ATeMdxtGUqS9gM+C8wd5qa02v5kQ05dZEeLd0maGhHPDmurhtb5wJKIuE7SbwA3Szop\nIn453A1rByPxSKKWn/p4tYyk/ckOUXe0pHVDo6afN5H0e8BfAO+JiJda1LahUq3PhwEnASVJm8nG\nble1+cnrWl7nrcCqiPhFRDwG/CdZ0mhXtfR5HrACICL+AziY7Ifw9lVN/TmjkZgkavmpj1XAnDR9\nLnBHpDNCbapqnyWdDPwDWYJo93FqqNLniNgdEWMjYlJETCI7D/OeiFg/PM1tilr27X8mO4pA0liy\n4adNrWxkk9XS558CZwJIeitZkvhZS1vZWquAi9JVTtOB3RHxZL0LG3HDTVHwUx+SrgLWR8Qq4Cay\nQ9IeshNEs4evxY2rsc//FzgU+Kd0jv6nEfGeYWt0g2rs8z6lxj6vAWZIehh4BfjziGjbo+Qa+7wA\n+JKkPyM7iT23nT/0Sfo6WaIfm86zXAEcABARXyQ77zIT6AFeAC5uaH1tvK3MzGyIjcThJjMzq5GT\nhJmZFXKSMDOzQk4SZmZWyEnCzMwKOUmYmVkhJwkzMyv0/wFkgCurLYrEXgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}