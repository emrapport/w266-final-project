{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperparam_experiments_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emrapport/w266-final-project/blob/master/hyperparam_experiments_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVfEbx0cZNNp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zHeoGlIX2sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# name of subfolder under models/ where trained models should go\n",
        "MODEL_PREFIX = \"test2\"\n",
        "\n",
        "hyp_combos = [{'NUM_EPOCHS': 2,\n",
        "                'BATCH_SIZE': 1000,\n",
        "                'MAX_SEQUENCE_LENGTH': 20,\n",
        "                'N_MOST_FREQ_WORDS_TO_KEEP': 5000,\n",
        "                'MAX_RESPONSES_PER_POST': 50,\n",
        "                'NUM_LAYERS': 2,\n",
        "                'CONV_LAYER_SIZES': [128, 64],\n",
        "                'FILTER_SIZES': [3, 2],\n",
        "                'DROPOUT_RATES': [.2, .2],\n",
        "                'FINAL_DENSE_LAYER_SIZE': 10,\n",
        "                # embed dim needs to map to one of the glove versions: 50, 100, 200, 300 \n",
        "                'EMBEDDING_DIM': 50},\n",
        "              \n",
        "                {'NUM_EPOCHS': 3,\n",
        "                'BATCH_SIZE': 1000,\n",
        "                'MAX_SEQUENCE_LENGTH': 20,\n",
        "                'N_MOST_FREQ_WORDS_TO_KEEP': 5000,\n",
        "                'MAX_RESPONSES_PER_POST': 10,\n",
        "                'NUM_LAYERS': 3,\n",
        "                'CONV_LAYER_SIZES': [128, 64, 32],\n",
        "                'FILTER_SIZES': [3, 2, 3],\n",
        "                'DROPOUT_RATES': [.2, .2, .2],\n",
        "                'FINAL_DENSE_LAYER_SIZE': 10,\n",
        "                # embed dim needs to map to one of the glove versions: 50, 100, 200, 300 \n",
        "                'EMBEDDING_DIM': 50}\n",
        "                ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGfRSm8JdMvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## check to ensure hyps are in agreement\n",
        "for hyp_combo in hyp_combos:\n",
        "  assert hyp_combo['NUM_LAYERS'] == len(hyp_combo['CONV_LAYER_SIZES'])\n",
        "  assert hyp_combo['NUM_LAYERS'] == len(hyp_combo['FILTER_SIZES'])\n",
        "  assert hyp_combo['NUM_LAYERS'] == len(hyp_combo['DROPOUT_RATES'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s79t_aJZsYG",
        "colab_type": "code",
        "outputId": "2590a8a7-f114-426d-c586-7d8c1776ac04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "## all this stuff just needs to get run one time per notebook\n",
        "# Set seeds for reproducible results.\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from scipy.sparse import hstack, vstack\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import Sequential, layers\n",
        "from keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "import time\n",
        "import pickle\n",
        "!pip install gcsfs\n",
        "\n",
        "pd.set_option('max_colwidth', 100)\n",
        "\n",
        "project_id = 'w266-251323'\n",
        "import uuid\n",
        "bucket_name = 'fb-congressional-data/'\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcloud config set project {project_id}\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.21.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.4.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.1)\n",
            "Requirement already satisfied: fsspec>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.5.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (3.1.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.12.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.7)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n",
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OugAe-Vx04Pf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "a16b8f7d-c9b5-4cae-95c9-fbedc562476e"
      },
      "source": [
        "train_df = pd.read_csv(\"gs://fb-congressional-data/contraction_expanded_data/train.csv\", index_col=0)\n",
        "dev_df = pd.read_csv(\"gs://fb-congressional-data/contraction_expanded_data/dev.csv\", index_col=0)\n",
        "!gsutil cp gs://fb-congressional-data/glove* /tmp/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://fb-congressional-data/glove.6B.100d.txt...\n",
            "Copying gs://fb-congressional-data/glove.6B.200d.txt...\n",
            "Copying gs://fb-congressional-data/glove.6B.300d.txt...\n",
            "Copying gs://fb-congressional-data/glove.6B.50d.txt...\n",
            "\\ [4 files][  2.1 GiB/  2.1 GiB]   66.6 MiB/s                                   \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://fb-congressional-data/glove.6B.zip...\n",
            "- [5 files][  2.9 GiB/  2.9 GiB]   22.3 MiB/s                                   \n",
            "Operation completed over 5 objects/2.9 GiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysaWA5e_aGLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all the functions go here\n",
        "def remove_excess_rows_per_post(df, max_per_post):\n",
        "  num_responses_per_post = df.post_id.value_counts().reset_index()\n",
        "  num_responses_per_post.columns = ['post_id', 'num_responses']\n",
        "  \n",
        "  too_big_posts = num_responses_per_post[num_responses_per_post.num_responses > max_per_post]\n",
        "  posts_to_sample = too_big_posts.post_id.values\n",
        "  \n",
        "  # this gets all the rows for posts we DON'T need to sample \n",
        "  new_train_df = df[~df.post_id.isin(posts_to_sample)]\n",
        "  # this should be true\n",
        "  assert(len(too_big_posts) + new_train_df.post_id.nunique() == df.post_id.nunique())\n",
        "  \n",
        "  too_big_post_rows = df[df.post_id.isin(posts_to_sample)]\n",
        "  sampled_rows = too_big_post_rows.groupby('post_id').apply(lambda x: x.sample(n=max_per_post)).reset_index(drop=True)\n",
        "  new_train_df = pd.concat([new_train_df, sampled_rows])\n",
        "  \n",
        "  return new_train_df\n",
        "\n",
        "def get_labels(train_df, test_df, party_label_ind):\n",
        "\n",
        "  def turn_to_ints(li):\n",
        "    final_list = []\n",
        "    for gender in li:\n",
        "        if gender=='M':\n",
        "            final_list.append(1)\n",
        "        else:\n",
        "            final_list.append(0)\n",
        "    return final_list\n",
        "\n",
        "  def turn_to_ints_party(li):\n",
        "    final_list = []\n",
        "    for party in li:\n",
        "        if party=='Congress_Republican':\n",
        "            final_list.append(1)\n",
        "        else:\n",
        "            final_list.append(0)\n",
        "    return final_list\n",
        "\n",
        "  if party_label_ind:\n",
        "    train_df = train_df[train_df.op_category!='Congress_Independent']\n",
        "\n",
        "    y_train = train_df.op_category.values\n",
        "    y_dev = test_df.op_category.values\n",
        "    y_train = turn_to_ints_party(y_train)\n",
        "    y_dev = turn_to_ints_party(y_dev) \n",
        "\n",
        "  else:\n",
        "    y_train = train_df.op_gender.values\n",
        "    y_dev = test_df.op_gender.values\n",
        "    y_train = turn_to_ints(y_train)\n",
        "    y_dev = turn_to_ints(y_dev)\n",
        "\n",
        "  y_train = np.asarray(y_train)\n",
        "  y_dev = np.asarray(y_dev)\n",
        "\n",
        "  return y_train, y_dev\n",
        "\n",
        "def get_inputs(train_df, \n",
        "               test_df, \n",
        "               n_words_to_keep,\n",
        "               max_seq_length):\n",
        "  def get_text_list(init_list):\n",
        "      sentences = []\n",
        "      for sentence in init_list:\n",
        "          if type(sentence) != str:\n",
        "              sentences.append(\"\")\n",
        "          else:\n",
        "              sentences.append(sentence)\n",
        "      return sentences\n",
        "\n",
        "  new_sentences_train = get_text_list(new_train_df.response_text.values)\n",
        "  new_sentences_test = get_text_list(dev_df.response_text.values)\n",
        "\n",
        "  time_start = time.time()\n",
        "\n",
        "  # this is the default list of filters + apostrophe\n",
        "  # added because we have dealt with common contractions, so other apostrophes should mostly be possessive \n",
        "  tokenizer = Tokenizer(filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', oov_token='UNK')\n",
        "  tokenizer.fit_on_texts(new_sentences_train)\n",
        "\n",
        "\n",
        "  currentTime = time.gmtime(time.time() - time_start)\n",
        "\n",
        "  #Convert the gmtime struct to a string\n",
        "  timeStr = time.strftime(\"%M minutes, %S seconds\", currentTime)\n",
        "\n",
        "  print(\"Tokenized in {}\".format(timeStr))\n",
        "\n",
        "  # suggestion from this issue: https://github.com/keras-team/keras/issues/8092\n",
        "  # seems like OOV and num_words don't work correctly by default \n",
        "  tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() \n",
        "                              if i <= n_words_to_keep + 1} \n",
        "  tokenizer.word_index[tokenizer.oov_token] = n_words_to_keep + 1\n",
        "\n",
        "  X_train = tokenizer.texts_to_sequences(new_sentences_train)\n",
        "  X_test = tokenizer.texts_to_sequences(new_sentences_test)\n",
        "\n",
        "  X_train = pad_sequences(X_train, padding='post', maxlen=max_seq_length)\n",
        "  X_test = pad_sequences(X_test, padding='post', maxlen=max_seq_length)\n",
        "  return X_train, X_test, tokenizer\n",
        "\n",
        "def create_embedding_matrix(filepath, \n",
        "                            word_index, \n",
        "                            embedding_dim):\n",
        "    vocab_size = len(word_index) + 2  # Now we have to add 2 (reserved 0 plus the manual UNK token)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "def make_model(embedding_matrix, \n",
        "               max_seq_length,\n",
        "               num_layers,\n",
        "               conv_layer_size,\n",
        "               filter_size,\n",
        "               dropout_rate,\n",
        "               final_hidden_dense_size):\n",
        "  model = Sequential()\n",
        "\n",
        "  try:\n",
        "    model.add(layers.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], \n",
        "                              weights=[embedding_matrix], \n",
        "                              input_length=max_seq_length, \n",
        "                              trainable=False))\n",
        "    for i in range(num_layers):\n",
        "      model.add(layers.Conv1D(conv_layer_size[i], filter_size[i], activation='relu', padding=\"same\"))\n",
        "      model.add(layers.Dropout(dropout_rate[i]))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(final_hidden_dense_size, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "  except Exception as ex:\n",
        "    print(ex)\n",
        "  finally:\n",
        "    return model\n",
        "\n",
        "def train_model(model, \n",
        "                train_labels,\n",
        "                dev_labels,\n",
        "                num_epochs,\n",
        "                batch_size):\n",
        "  try:\n",
        "    time_start = time.time()\n",
        "\n",
        "    history = model.fit(X_train, train_labels,\n",
        "                        epochs=num_epochs,\n",
        "                        verbose=True,\n",
        "                        validation_data=(X_test, dev_labels),\n",
        "                        batch_size=batch_size)\n",
        "\n",
        "    currentTime = time.gmtime(time.time() - time_start)\n",
        "\n",
        "    #Convert the gmtime struct to a string\n",
        "    timeStr = time.strftime(\"%M minutes, %S seconds\", currentTime)\n",
        "\n",
        "    print(\"Trained in {}\".format(timeStr))\n",
        "\n",
        "  except Exception as ex:\n",
        "    print(\"Exception: {}\".format(ex))\n",
        "    currentTime = time.gmtime(time.time() - time_start)\n",
        "\n",
        "    #Convert the gmtime struct to a string\n",
        "    timeStr = time.strftime(\"%M minutes, %S seconds\", currentTime)\n",
        "\n",
        "    print(\"Trained in {}\".format(timeStr))  \n",
        "  finally:\n",
        "    return model\n",
        "\n",
        "def pred_to_label(row):\n",
        "  if row['probs'] >= .5:\n",
        "    return 'M'\n",
        "  else:\n",
        "    return 'W'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s844qbPgZi2s",
        "colab_type": "code",
        "outputId": "b2513ad8-4e9b-439c-f7bc-56f3ca42f4a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i, hyp_dict in enumerate(hyp_combos):\n",
        "  print(\"Training Model {}\".format(i))\n",
        "  new_train_df = remove_excess_rows_per_post(train_df, hyp_dict['MAX_RESPONSES_PER_POST'])\n",
        "  print(\"Limiting by responses per post resulted in {} training rows\".format(new_train_df.shape[0]))\n",
        "\n",
        "  # TODO: remove this line after testing\n",
        "  new_train_df = new_train_df[:10000]\n",
        "\n",
        "  new_train_df = new_train_df.sample(frac=1)\n",
        "  dev_df = dev_df.sample(frac=1)\n",
        "\n",
        "  # get two sets of labels: gender and party\n",
        "  print(\"Getting labels\")\n",
        "  y_train_gender, y_dev_gender = get_labels(new_train_df, dev_df, False)\n",
        "  y_train_party, y_dev_party = get_labels(new_train_df, dev_df, True)\n",
        "\n",
        "  print(\"Getting inputs\")\n",
        "  X_train, X_test, tokenizer = get_inputs(new_train_df, \n",
        "                                          dev_df, \n",
        "                                          hyp_dict['N_MOST_FREQ_WORDS_TO_KEEP'], \n",
        "                                          hyp_dict['MAX_SEQUENCE_LENGTH'])\n",
        "  embedding_matrix = create_embedding_matrix(\n",
        "                     '/tmp/glove.6B.{}d.txt'.format(hyp_dict['EMBEDDING_DIM']),\n",
        "                      tokenizer.word_index, hyp_dict['EMBEDDING_DIM'])\n",
        "  \n",
        "  # gender model\n",
        "  model = make_model(embedding_matrix, \n",
        "                     hyp_dict['MAX_SEQUENCE_LENGTH'],\n",
        "                     hyp_dict['NUM_LAYERS'],\n",
        "                     hyp_dict['CONV_LAYER_SIZES'],\n",
        "                     hyp_dict['FILTER_SIZES'],\n",
        "                     hyp_dict['DROPOUT_RATES'],\n",
        "                     hyp_dict['FINAL_DENSE_LAYER_SIZE'])\n",
        "  print(model.summary())\n",
        "  trained_model = train_model(model,\n",
        "                              y_train_gender,\n",
        "                              y_dev_gender,\n",
        "                              hyp_dict['NUM_EPOCHS'],\n",
        "                              hyp_dict['BATCH_SIZE']\n",
        "                              )\n",
        "  \n",
        "  # adding epoch time just in case we accidentally re-use the same prefixes \n",
        "  gender_model_path = '{}_model_{}_gender_{}.h5'.format(MODEL_PREFIX, i, time.time())\n",
        "  model.save(gender_model_path)\n",
        "  !gsutil cp /content/{gender_model_path} gs://fb-congressional-data/models/{gender_model_path}\n",
        "\n",
        "  preds = model.predict(X_test)\n",
        "  dev_df['probs'] = preds\n",
        "  dev_df['preds'] = dev_df.apply(pred_to_label, axis=1)\n",
        "\n",
        "  if 'W' in dev_df.preds.value_counts():\n",
        "    proportion_women_predicted = dev_df.preds.value_counts()['W'] / len(dev_df)\n",
        "  else:\n",
        "    proportion_women_predicted = 0\n",
        "  print(\"Proportion of predictions for W class: {}\".format(proportion_women_predicted))\n",
        "  plt.figure()\n",
        "  print(dev_df.probs.hist(bins=20))\n",
        "\n",
        "  # party model\n",
        "  model2 = make_model(embedding_matrix, \n",
        "                      hyp_dict['MAX_SEQUENCE_LENGTH'],\n",
        "                      hyp_dict['NUM_LAYERS'],\n",
        "                      hyp_dict['CONV_LAYER_SIZES'],\n",
        "                      hyp_dict['FILTER_SIZES'],\n",
        "                      hyp_dict['DROPOUT_RATES'],\n",
        "                      hyp_dict['FINAL_DENSE_LAYER_SIZE'])\n",
        "  trained_model2 = train_model(model2,\n",
        "                               y_train_party,\n",
        "                               y_dev_party,\n",
        "                               hyp_dict['NUM_EPOCHS'],\n",
        "                               hyp_dict['BATCH_SIZE'])\n",
        "  \n",
        "  party_model_path = '{}_model_{}_party.h5'.format(MODEL_PREFIX, i)\n",
        "  model.save(party_model_path)\n",
        "  !gsutil cp /content/{party_model_path} gs://fb-congressional-data/models/{party_model_path}\n",
        " \n",
        "  \n",
        "  \n",
        "    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Model 0\n",
            "Limiting by responses per post resulted in 3962284 training rows\n",
            "Getting labels\n",
            "Getting inputs\n",
            "Tokenized in 00 minutes, 00 seconds\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, 20, 50)            250150    \n",
            "_________________________________________________________________\n",
            "conv1d_22 (Conv1D)           (None, 20, 128)           19328     \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 20, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_23 (Conv1D)           (None, 20, 64)            16448     \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 20, 64)            0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_9 (Glob (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 286,587\n",
            "Trainable params: 36,437\n",
            "Non-trainable params: 250,150\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 10000 samples, validate on 2292907 samples\n",
            "Epoch 1/2\n",
            "10000/10000 [==============================] - 67s 7ms/sample - loss: 0.0777 - acc: 0.9992 - val_loss: 1.0316 - val_acc: 0.8497\n",
            "Epoch 2/2\n",
            "10000/10000 [==============================] - 67s 7ms/sample - loss: 0.0059 - acc: 1.0000 - val_loss: 1.5584 - val_acc: 0.8497\n",
            "Trained in 02 minutes, 14 seconds\n",
            "Copying file:///content/test2_model_0_gender_1573433850.3795247.h5 [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/1.4 MiB.                                      \n",
            "Proportion of predictions for W class: 0\n",
            "AxesSubplot(0.125,0.125;0.775x0.755)\n",
            "Train on 10000 samples, validate on 2292907 samples\n",
            "Epoch 1/2\n",
            "10000/10000 [==============================] - 65s 7ms/sample - loss: 0.1289 - acc: 0.9888 - val_loss: 1.4661 - val_acc: 0.7407\n",
            "Epoch 2/2\n",
            "10000/10000 [==============================] - 64s 6ms/sample - loss: 0.0115 - acc: 1.0000 - val_loss: 2.2383 - val_acc: 0.7407\n",
            "Trained in 02 minutes, 10 seconds\n",
            "Copying file:///content/test2_model_0_party.h5 [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/1.4 MiB.                                      \n",
            "Training Model 1\n",
            "Limiting by responses per post resulted in 1754816 training rows\n",
            "Getting labels\n",
            "Getting inputs\n",
            "Tokenized in 00 minutes, 00 seconds\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 20, 50)            250150    \n",
            "_________________________________________________________________\n",
            "conv1d_26 (Conv1D)           (None, 20, 128)           19328     \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 20, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_27 (Conv1D)           (None, 20, 64)            16448     \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 20, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_28 (Conv1D)           (None, 20, 32)            6176      \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 20, 32)            0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_11 (Glo (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 292,443\n",
            "Trainable params: 42,293\n",
            "Non-trainable params: 250,150\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 10000 samples, validate on 2292907 samples\n",
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 78s 8ms/sample - loss: 0.7375 - acc: 0.6225 - val_loss: 0.5006 - val_acc: 0.8497\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 77s 8ms/sample - loss: 0.1355 - acc: 1.0000 - val_loss: 0.5332 - val_acc: 0.8497\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 78s 8ms/sample - loss: 0.0189 - acc: 1.0000 - val_loss: 0.9670 - val_acc: 0.8497\n",
            "Trained in 03 minutes, 54 seconds\n",
            "Copying file:///content/test2_model_1_gender_1573434604.9503746.h5 [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/1.5 MiB.                                      \n",
            "Proportion of predictions for W class: 0\n",
            "AxesSubplot(0.125,0.125;0.775x0.755)\n",
            "Train on 10000 samples, validate on 2292907 samples\n",
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 79s 8ms/sample - loss: 0.6316 - acc: 0.6387 - val_loss: 0.7471 - val_acc: 0.4538\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 78s 8ms/sample - loss: 0.5931 - acc: 0.6731 - val_loss: 0.7869 - val_acc: 0.5305\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 80s 8ms/sample - loss: 0.5722 - acc: 0.6945 - val_loss: 0.7956 - val_acc: 0.5286\n",
            "Trained in 03 minutes, 59 seconds\n",
            "Copying file:///content/test2_model_1_party.h5 [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/1.5 MiB.                                      \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAD4CAYAAADCb7BPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npC4GiCSpiwEiSepigEiSuvwfPY/Pthc8PKsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAD4CAYAAADCb7BPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlYdhAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYZ83nfLXvhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aItuYkpo7N-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}