{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperparam_experiments_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emrapport/w266-final-project/blob/master/hyperparam_experiments_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVfEbx0cZNNp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zHeoGlIX2sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# name of subfolder under models/ where trained models should go\n",
        "MODEL_PREFIX = \"test2\"\n",
        "\n",
        "hyp_combos = [{'NUM_EPOCHS': 2,\n",
        "                'BATCH_SIZE': 1000,\n",
        "                'MAX_SEQUENCE_LENGTH': 20,\n",
        "                'N_MOST_FREQ_WORDS_TO_KEEP': 5000,\n",
        "                'MAX_RESPONSES_PER_POST': 50,\n",
        "                'NUM_LAYERS': 2,\n",
        "                'CONV_LAYER_SIZES': [128, 64],\n",
        "                'FILTER_SIZES': [3, 2],\n",
        "                'DROPOUT_RATES': [.2, .2],\n",
        "                'FINAL_DENSE_LAYER_SIZE': 10,\n",
        "                # embed dim needs to map to one of the glove versions: 50, 100, 200, 300 \n",
        "                'EMBEDDING_DIM': 50},\n",
        "              \n",
        "                {'NUM_EPOCHS': 3,\n",
        "                'BATCH_SIZE': 1000,\n",
        "                'MAX_SEQUENCE_LENGTH': 20,\n",
        "                'N_MOST_FREQ_WORDS_TO_KEEP': 5000,\n",
        "                'MAX_RESPONSES_PER_POST': 10,\n",
        "                'NUM_LAYERS': 3,\n",
        "                'CONV_LAYER_SIZES': [128, 64, 32],\n",
        "                'FILTER_SIZES': [3, 2, 3],\n",
        "                'DROPOUT_RATES': [.2, .2, .2],\n",
        "                'FINAL_DENSE_LAYER_SIZE': 10,\n",
        "                # embed dim needs to map to one of the glove versions: 50, 100, 200, 300 \n",
        "                'EMBEDDING_DIM': 50}\n",
        "                ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGfRSm8JdMvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## check to ensure hyps are in agreement\n",
        "for hyp_combo in hyp_combos:\n",
        "  assert hyp_combo['NUM_LAYERS'] == len(hyp_combo['CONV_LAYER_SIZES'])\n",
        "  assert hyp_combo['NUM_LAYERS'] == len(hyp_combo['FILTER_SIZES'])\n",
        "  assert hyp_combo['NUM_LAYERS'] == len(hyp_combo['DROPOUT_RATES'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s79t_aJZsYG",
        "colab_type": "code",
        "outputId": "2590a8a7-f114-426d-c586-7d8c1776ac04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "## all this stuff just needs to get run one time per notebook\n",
        "# Set seeds for reproducible results.\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from scipy.sparse import hstack, vstack\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import Sequential, layers\n",
        "from keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "import time\n",
        "import pickle\n",
        "!pip install gcsfs\n",
        "\n",
        "pd.set_option('max_colwidth', 100)\n",
        "\n",
        "project_id = 'w266-251323'\n",
        "import uuid\n",
        "bucket_name = 'fb-congressional-data/'\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcloud config set project {project_id}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.21.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.4.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.1)\n",
            "Requirement already satisfied: fsspec>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.5.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (3.1.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.12.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.7)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n",
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OugAe-Vx04Pf",
        "colab_type": "code",
        "outputId": "a16b8f7d-c9b5-4cae-95c9-fbedc562476e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "train_df = pd.read_csv(\"gs://fb-congressional-data/contraction_expanded_data/train.csv\", index_col=0)\n",
        "dev_df = pd.read_csv(\"gs://fb-congressional-data/contraction_expanded_data/dev.csv\", index_col=0)\n",
        "!gsutil cp gs://fb-congressional-data/glove* /tmp/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://fb-congressional-data/glove.6B.100d.txt...\n",
            "Copying gs://fb-congressional-data/glove.6B.200d.txt...\n",
            "Copying gs://fb-congressional-data/glove.6B.300d.txt...\n",
            "Copying gs://fb-congressional-data/glove.6B.50d.txt...\n",
            "\\ [4 files][  2.1 GiB/  2.1 GiB]   66.6 MiB/s                                   \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://fb-congressional-data/glove.6B.zip...\n",
            "- [5 files][  2.9 GiB/  2.9 GiB]   22.3 MiB/s                                   \n",
            "Operation completed over 5 objects/2.9 GiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysaWA5e_aGLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all the functions go here\n",
        "def remove_excess_rows_per_post(df, max_per_post):\n",
        "  num_responses_per_post = df.post_id.value_counts().reset_index()\n",
        "  num_responses_per_post.columns = ['post_id', 'num_responses']\n",
        "  \n",
        "  too_big_posts = num_responses_per_post[num_responses_per_post.num_responses > max_per_post]\n",
        "  posts_to_sample = too_big_posts.post_id.values\n",
        "  \n",
        "  # this gets all the rows for posts we DON'T need to sample \n",
        "  new_train_df = df[~df.post_id.isin(posts_to_sample)]\n",
        "  # this should be true\n",
        "  assert(len(too_big_posts) + new_train_df.post_id.nunique() == df.post_id.nunique())\n",
        "  \n",
        "  too_big_post_rows = df[df.post_id.isin(posts_to_sample)]\n",
        "  sampled_rows = too_big_post_rows.groupby('post_id').apply(lambda x: x.sample(n=max_per_post)).reset_index(drop=True)\n",
        "  new_train_df = pd.concat([new_train_df, sampled_rows])\n",
        "  \n",
        "  return new_train_df\n",
        "\n",
        "def get_labels(train_df, test_df, party_label_ind):\n",
        "\n",
        "  def turn_to_ints(li):\n",
        "    final_list = []\n",
        "    for gender in li:\n",
        "        if gender=='M':\n",
        "            final_list.append(1)\n",
        "        else:\n",
        "            final_list.append(0)\n",
        "    return final_list\n",
        "\n",
        "  def turn_to_ints_party(li):\n",
        "    final_list = []\n",
        "    for party in li:\n",
        "        if party=='Congress_Republican':\n",
        "            final_list.append(1)\n",
        "        else:\n",
        "            final_list.append(0)\n",
        "    return final_list\n",
        "\n",
        "  if party_label_ind:\n",
        "    train_df = train_df[train_df.op_category!='Congress_Independent']\n",
        "\n",
        "    y_train = train_df.op_category.values\n",
        "    y_dev = test_df.op_category.values\n",
        "    y_train = turn_to_ints_party(y_train)\n",
        "    y_dev = turn_to_ints_party(y_dev) \n",
        "\n",
        "  else:\n",
        "    y_train = train_df.op_gender.values\n",
        "    y_dev = test_df.op_gender.values\n",
        "    y_train = turn_to_ints(y_train)\n",
        "    y_dev = turn_to_ints(y_dev)\n",
        "\n",
        "  y_train = np.asarray(y_train)\n",
        "  y_dev = np.asarray(y_dev)\n",
        "\n",
        "  return y_train, y_dev\n",
        "\n",
        "def get_inputs(train_df, \n",
        "               test_df, \n",
        "               n_words_to_keep,\n",
        "               max_seq_length):\n",
        "  def get_text_list(init_list):\n",
        "      sentences = []\n",
        "      for sentence in init_list:\n",
        "          if type(sentence) != str:\n",
        "              sentences.append(\"\")\n",
        "          else:\n",
        "              sentences.append(sentence)\n",
        "      return sentences\n",
        "\n",
        "  new_sentences_train = get_text_list(new_train_df.response_text.values)\n",
        "  new_sentences_test = get_text_list(dev_df.response_text.values)\n",
        "\n",
        "  time_start = time.time()\n",
        "\n",
        "  # this is the default list of filters + apostrophe\n",
        "  # added because we have dealt with common contractions, so other apostrophes should mostly be possessive \n",
        "  tokenizer = Tokenizer(filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', oov_token='UNK')\n",
        "  tokenizer.fit_on_texts(new_sentences_train)\n",
        "\n",
        "\n",
        "  currentTime = time.gmtime(time.time() - time_start)\n",
        "\n",
        "  #Convert the gmtime struct to a string\n",
        "  timeStr = time.strftime(\"%M minutes, %S seconds\", currentTime)\n",
        "\n",
        "  print(\"Tokenized in {}\".format(timeStr))\n",
        "\n",
        "  # suggestion from this issue: https://github.com/keras-team/keras/issues/8092\n",
        "  # seems like OOV and num_words don't work correctly by default \n",
        "  tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() \n",
        "                              if i <= n_words_to_keep + 1} \n",
        "  tokenizer.word_index[tokenizer.oov_token] = n_words_to_keep + 1\n",
        "\n",
        "  X_train = tokenizer.texts_to_sequences(new_sentences_train)\n",
        "  X_test = tokenizer.texts_to_sequences(new_sentences_test)\n",
        "\n",
        "  X_train = pad_sequences(X_train, padding='post', maxlen=max_seq_length)\n",
        "  X_test = pad_sequences(X_test, padding='post', maxlen=max_seq_length)\n",
        "  return X_train, X_test, tokenizer\n",
        "\n",
        "def create_embedding_matrix(filepath, \n",
        "                            word_index, \n",
        "                            embedding_dim):\n",
        "    vocab_size = len(word_index) + 2  # Now we have to add 2 (reserved 0 plus the manual UNK token)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "def make_model(embedding_matrix, \n",
        "               max_seq_length,\n",
        "               num_layers,\n",
        "               conv_layer_size,\n",
        "               filter_size,\n",
        "               dropout_rate,\n",
        "               final_hidden_dense_size):\n",
        "  model = Sequential()\n",
        "\n",
        "  try:\n",
        "    model.add(layers.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], \n",
        "                              weights=[embedding_matrix], \n",
        "                              input_length=max_seq_length, \n",
        "                              trainable=False))\n",
        "    for i in range(num_layers):\n",
        "      model.add(layers.Conv1D(conv_layer_size[i], filter_size[i], activation='relu', padding=\"same\"))\n",
        "      model.add(layers.Dropout(dropout_rate[i]))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(final_hidden_dense_size, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "  except Exception as ex:\n",
        "    print(ex)\n",
        "  finally:\n",
        "    return model\n",
        "\n",
        "def train_model(model, \n",
        "                train_labels,\n",
        "                dev_labels,\n",
        "                num_epochs,\n",
        "                batch_size):\n",
        "  try:\n",
        "    time_start = time.time()\n",
        "\n",
        "    history = model.fit(X_train, train_labels,\n",
        "                        epochs=num_epochs,\n",
        "                        verbose=True,\n",
        "                        validation_data=(X_test, dev_labels),\n",
        "                        batch_size=batch_size)\n",
        "\n",
        "    currentTime = time.gmtime(time.time() - time_start)\n",
        "\n",
        "    #Convert the gmtime struct to a string\n",
        "    timeStr = time.strftime(\"%M minutes, %S seconds\", currentTime)\n",
        "\n",
        "    print(\"Trained in {}\".format(timeStr))\n",
        "\n",
        "  except Exception as ex:\n",
        "    print(\"Exception: {}\".format(ex))\n",
        "    currentTime = time.gmtime(time.time() - time_start)\n",
        "\n",
        "    #Convert the gmtime struct to a string\n",
        "    timeStr = time.strftime(\"%M minutes, %S seconds\", currentTime)\n",
        "\n",
        "    print(\"Trained in {}\".format(timeStr))  \n",
        "  finally:\n",
        "    return model\n",
        "\n",
        "def pred_to_label(row):\n",
        "  if row['probs'] >= .5:\n",
        "    return 'M'\n",
        "  else:\n",
        "    return 'W'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s844qbPgZi2s",
        "colab_type": "code",
        "outputId": "405e40a3-7d1d-43f1-e9f3-e569b28abf17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# timestamp is shared across all runs\n",
        "timestamp = time.time()\n",
        "for i, hyp_dict in enumerate(hyp_combos):\n",
        "  print(\"Training Model {}\".format(i))\n",
        "  new_train_df = remove_excess_rows_per_post(train_df, hyp_dict['MAX_RESPONSES_PER_POST'])\n",
        "  print(\"Limiting by responses per post resulted in {} training rows\".format(new_train_df.shape[0]))\n",
        "\n",
        "  # TODO: remove this line after testing\n",
        "  new_train_df = new_train_df[:10000]\n",
        "\n",
        "  new_train_df = new_train_df.sample(frac=1)\n",
        "  dev_df = dev_df.sample(frac=1)\n",
        "\n",
        "  # get two sets of labels: gender and party\n",
        "  print(\"Getting labels\")\n",
        "  y_train_gender, y_dev_gender = get_labels(new_train_df, dev_df, False)\n",
        "  y_train_party, y_dev_party = get_labels(new_train_df, dev_df, True)\n",
        "\n",
        "  print(\"Getting inputs\")\n",
        "  X_train, X_test, tokenizer = get_inputs(new_train_df, \n",
        "                                          dev_df, \n",
        "                                          hyp_dict['N_MOST_FREQ_WORDS_TO_KEEP'], \n",
        "                                          hyp_dict['MAX_SEQUENCE_LENGTH'])\n",
        "  embedding_matrix = create_embedding_matrix(\n",
        "                     '/tmp/glove.6B.{}d.txt'.format(hyp_dict['EMBEDDING_DIM']),\n",
        "                      tokenizer.word_index, hyp_dict['EMBEDDING_DIM'])\n",
        "  \n",
        "  # gender model\n",
        "  model = make_model(embedding_matrix, \n",
        "                     hyp_dict['MAX_SEQUENCE_LENGTH'],\n",
        "                     hyp_dict['NUM_LAYERS'],\n",
        "                     hyp_dict['CONV_LAYER_SIZES'],\n",
        "                     hyp_dict['FILTER_SIZES'],\n",
        "                     hyp_dict['DROPOUT_RATES'],\n",
        "                     hyp_dict['FINAL_DENSE_LAYER_SIZE'])\n",
        "  print(model.summary())\n",
        "  trained_model = train_model(model,\n",
        "                              y_train_gender,\n",
        "                              y_dev_gender,\n",
        "                              hyp_dict['NUM_EPOCHS'],\n",
        "                              hyp_dict['BATCH_SIZE']\n",
        "                              )\n",
        "  \n",
        "  # adding epoch time just in case we accidentally re-use the same prefixes \n",
        "  gender_model_path = '{}_model_{}_gender_{}.h5'.format(MODEL_PREFIX, i, timestamp)\n",
        "  model.save(gender_model_path)\n",
        "  !gsutil cp /content/{gender_model_path} gs://fb-congressional-data/models/{gender_model_path}\n",
        "\n",
        "  preds = model.predict(X_test)\n",
        "  dev_df['probs'] = preds\n",
        "  dev_df['preds'] = dev_df.apply(pred_to_label, axis=1)\n",
        "\n",
        "  if 'W' in dev_df.preds.value_counts():\n",
        "    proportion_women_predicted = dev_df.preds.value_counts()['W'] / len(dev_df)\n",
        "  else:\n",
        "    proportion_women_predicted = 0\n",
        "  print(\"Proportion of predictions for W class: {}\".format(proportion_women_predicted))\n",
        "  plt.figure()\n",
        "  print(dev_df.probs.hist(bins=20))\n",
        "\n",
        "  # party model\n",
        "  model2 = make_model(embedding_matrix, \n",
        "                      hyp_dict['MAX_SEQUENCE_LENGTH'],\n",
        "                      hyp_dict['NUM_LAYERS'],\n",
        "                      hyp_dict['CONV_LAYER_SIZES'],\n",
        "                      hyp_dict['FILTER_SIZES'],\n",
        "                      hyp_dict['DROPOUT_RATES'],\n",
        "                      hyp_dict['FINAL_DENSE_LAYER_SIZE'])\n",
        "  trained_model2 = train_model(model2,\n",
        "                               y_train_party,\n",
        "                               y_dev_party,\n",
        "                               hyp_dict['NUM_EPOCHS'],\n",
        "                               hyp_dict['BATCH_SIZE'])\n",
        "  \n",
        "  party_model_path = '{}_model_{}_party_{}.h5'.format(MODEL_PREFIX, i, timestamp)\n",
        "  model.save(party_model_path)\n",
        "  !gsutil cp /content/{party_model_path} gs://fb-congressional-data/models/{party_model_path}\n",
        " \n",
        "  \n",
        "  \n",
        "    "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Model 0\n",
            "Limiting by responses per post resulted in 3962284 training rows\n",
            "Getting labels\n",
            "Getting inputs\n",
            "Tokenized in 00 minutes, 00 seconds\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 20, 50)            250150    \n",
            "_________________________________________________________________\n",
            "conv1d_32 (Conv1D)           (None, 20, 128)           19328     \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 20, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_33 (Conv1D)           (None, 20, 64)            16448     \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 20, 64)            0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_13 (Glo (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 286,587\n",
            "Trainable params: 36,437\n",
            "Non-trainable params: 250,150\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 10000 samples, validate on 2292907 samples\n",
            "Epoch 1/2\n",
            "10000/10000 [==============================] - 66s 7ms/sample - loss: 0.0801 - acc: 0.9993 - val_loss: 0.9657 - val_acc: 0.8497\n",
            "Epoch 2/2\n",
            "10000/10000 [==============================] - 65s 7ms/sample - loss: 0.0064 - acc: 1.0000 - val_loss: 1.5884 - val_acc: 0.8497\n",
            "Trained in 02 minutes, 12 seconds\n",
            "Copying file:///content/test2_model_0_gender_1573436751.7996738.h5 [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/1.4 MiB.                                      \n",
            "Proportion of predictions for W class: 0\n",
            "AxesSubplot(0.125,0.125;0.775x0.755)\n",
            "Train on 10000 samples, validate on 2292907 samples\n",
            "Epoch 1/2\n",
            "10000/10000 [==============================] - 67s 7ms/sample - loss: 0.0690 - acc: 0.9985 - val_loss: 1.6631 - val_acc: 0.7407\n",
            "Epoch 2/2\n",
            "10000/10000 [==============================] - 66s 7ms/sample - loss: 0.0057 - acc: 1.0000 - val_loss: 2.5793 - val_acc: 0.7407\n",
            "Trained in 02 minutes, 13 seconds\n",
            "Copying file:///content/test2_model_0_party_1573436751.7996738.h5 [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/1.4 MiB.                                      \n",
            "Training Model 1\n",
            "Limiting by responses per post resulted in 1754816 training rows\n",
            "Getting labels\n",
            "Getting inputs\n",
            "Tokenized in 00 minutes, 00 seconds\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_15 (Embedding)     (None, 20, 50)            250150    \n",
            "_________________________________________________________________\n",
            "conv1d_36 (Conv1D)           (None, 20, 128)           19328     \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 20, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_37 (Conv1D)           (None, 20, 64)            16448     \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 20, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_38 (Conv1D)           (None, 20, 32)            6176      \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 20, 32)            0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_15 (Glo (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 292,443\n",
            "Trainable params: 42,293\n",
            "Non-trainable params: 250,150\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 10000 samples, validate on 2292907 samples\n",
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 77s 8ms/sample - loss: 0.4629 - acc: 0.7778 - val_loss: 0.4396 - val_acc: 0.8497\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 77s 8ms/sample - loss: 0.0307 - acc: 1.0000 - val_loss: 0.9332 - val_acc: 0.8497\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 78s 8ms/sample - loss: 0.0051 - acc: 1.0000 - val_loss: 1.4393 - val_acc: 0.8497\n",
            "Trained in 03 minutes, 54 seconds\n",
            "Copying file:///content/test2_model_1_gender_1573436751.7996738.h5 [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/1.5 MiB.                                      \n",
            "Proportion of predictions for W class: 0\n",
            "AxesSubplot(0.125,0.125;0.775x0.755)\n",
            "Train on 10000 samples, validate on 2292907 samples\n",
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 78s 8ms/sample - loss: 0.6294 - acc: 0.6466 - val_loss: 0.8032 - val_acc: 0.3484\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 77s 8ms/sample - loss: 0.5910 - acc: 0.6758 - val_loss: 0.8161 - val_acc: 0.4559\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 76s 8ms/sample - loss: 0.5696 - acc: 0.6999 - val_loss: 0.8076 - val_acc: 0.4731\n",
            "Trained in 03 minutes, 53 seconds\n",
            "Copying file:///content/test2_model_1_party_1573436751.7996738.h5 [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/1.5 MiB.                                      \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAD4CAYAAADCb7BPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXQwQSVIXA0SS1OX/AUpW2EiBShp9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAD4CAYAAADCb7BPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASsUlEQVR4nO3df6zddX3H8efbdqgpYovVG0LRsq1m\nQ8mU3kCzzXmRrVxwW9E5AstGwYbOifuRscQ6s2BAE/xjupEoW5WmrZl2zM3QDFjTVO6MZlWK/Cjg\noNeCobXSSAus6tSy9/44nyuHm3PvPedzz6/C85GcnO95f7/fz+d9vxzuq+f7/fY0MhNJkjr1skE3\nIEk6MRkgkqQqBogkqYoBIkmqYoBIkqosHHQD/bJ06dJcvnz5vMf5wQ9+wKJFi+bfUJfZV/uGsSew\nr04MY0/w4uzrnnvu+X5mvrblysx8STxWrlyZ3XDXXXd1ZZxus6/2DWNPmfbViWHsKfPF2RewJ2f4\nveopLElSFQNEklTFAJEkVTFAJElVDBBJUhUDRJJUxQCRJFUxQCRJVQwQSVKVl8xXmUjSiWz5htur\n99083puvV/ETiCSpigEiSapigEiSqhggkqQqBogkqYoBIkmqYoBIkqoYIJKkKgaIJKmKASJJqmKA\nSJKqGCCSpCoGiCSpigEiSapigEiSqswZIBFxRkTcFREPR8RDEfHnpX5qROyMiH3leUmpR0TcFBGT\nEfFARJzTNNbasv2+iFjbVF8ZEXvLPjdFRNTOIUnqj3Y+gRwHrs3Ms4BVwDURcRawAdiVmSuAXeU1\nwEXAivJYD9wMjTAArgPOA84FrpsKhLLN1U37jZd6R3NIkvpnzgDJzEOZ+c2y/D/At4DTgTXAlrLZ\nFuCSsrwG2JoNu4HFEXEacCGwMzOPZOZRYCcwXtadkpm7MzOBrdPG6mQOSVKfdPRP2kbEcuCtwNeB\nkcw8VFZ9Dxgpy6cDTzTtdqDUZqsfaFGnYo5DTTUiYj2NTyiMjIwwMTHR1s85m2PHjnVlnG6zr/YN\nY09gX50Yxp6gt31de/bx6n171VfbARIRJwP/CvxFZj5bLlMAkJkZEdn17prUzJGZG4GNAKOjozk2\nNjbvPiYmJujGON1mX+0bxp7AvjoxjD1Bb/u6cp7/Jnov+mrrLqyI+Dka4fFPmflvpfzk1Gmj8ny4\n1A8CZzTtvqzUZqsva1GvmUOS1Cft3IUVwC3AtzLzE02rtgNTd1KtBW5rql9R7pRaBTxTTkPtAFZH\nxJJy8Xw1sKOsezYiVpW5rpg2VidzSJL6pJ1TWL8G/BGwNyLuK7W/Bm4Ebo2IdcB3gEvLujuAi4FJ\n4IfAVQCZeSQibgDuLttdn5lHyvL7gc3AK4E7y4NO55Ak9c+cAZKZXwVihtUXtNg+gWtmGGsTsKlF\nfQ/w5hb1pzqdQ5LUH/5NdElSFQNEklTFAJEkVTFAJElVDBBJUhUDRJJUxQCRJFUxQCRJVQwQSVIV\nA0SSVMUAkSRVMUAkSVUMEElSFQNEklTFAJEkVTFAJElVDBBJUhUDRJJUxQCRJFUxQCRJVQwQSVIV\nA0SSVMUAkSRVMUAkSVUMEElSFQNEklTFAJEkVTFAJElVDBBJUhUDRJJUxQCRJFUxQCRJVQwQSVIV\nA0SSVMUAkSRVMUAkSVUMEElSFQNEklRlzgCJiE0RcTgiHmyqfSQiDkbEfeVxcdO6D0XEZEQ8EhEX\nNtXHS20yIjY01c+MiK+X+j9HxEml/vLyerKsXz7XHJKk/mnnE8hmYLxF/ZOZ+ZbyuAMgIs4CLgPe\nVPb5dEQsiIgFwKeAi4CzgMvLtgAfL2P9InAUWFfq64Cjpf7Jst2Mc3T2Y0uS5mvOAMnMrwBH2hxv\nDbAtM3+cmY8Bk8C55TGZmfsz8yfANmBNRATwDuCLZf8twCVNY20py18ELijbzzSHJKmPFs5j3w9E\nxBXAHuDazDwKnA7sbtrmQKkBPDGtfh7wGuDpzDzeYvvTp/bJzOMR8UzZfrY5XiAi1gPrAUZGRpiY\nmOj8p5zm2LFjXRmn2+yrfcPYE9hXJ4axJ+htX9eefXzujWbQq75qA+Rm4AYgy/PfAu/tVlPdkpkb\ngY0Ao6OjOTY2Nu8xJyYm6MY43WZf7RvGnsC+OjGMPUFv+7pyw+3V+24eX9STvqruwsrMJzPzucz8\nP+AzPH8K6SBwRtOmy0ptpvpTwOKIWDit/oKxyvpXl+1nGkuS1EdVARIRpzW9fBcwdYfWduCycgfV\nmcAK4BvA3cCKcsfVSTQugm/PzATuAt5T9l8L3NY01tqy/B7gy2X7meaQJPXRnKewIuILwBiwNCIO\nANcBYxHxFhqnsB4H/hggMx+KiFuBh4HjwDWZ+VwZ5wPADmABsCkzHypTfBDYFhEfBe4Fbin1W4DP\nRcQkjYv4l801hySpf+YMkMy8vEX5lha1qe0/BnysRf0O4I4W9f20uIsqM/8X+P1O5pAk9Y9/E12S\nVMUAkSRVMUAkSVUMEElSFQNEklTFAJEkVTFAJElVDBBJUhUDRJJUxQCRJFUxQCRJVQwQSVIVA0SS\nVMUAkSRVMUAkSVUMEElSFQNEklTFAJEkVTFAJElVDBBJUhUDRJJUxQCRJFUxQCRJVQwQSVIVA0SS\nVMUAkSRVMUAkSVUMEElSFQNEklTFAJEkVTFAJElVDBBJUhUDRJJUxQCRJFUxQCRJVQwQSVIVA0SS\nVMUAkSRVmTNAImJTRByOiAebaqdGxM6I2Feel5R6RMRNETEZEQ9ExDlN+6wt2++LiLVN9ZURsbfs\nc1NERO0ckqT+aecTyGZgfFptA7ArM1cAu8prgIuAFeWxHrgZGmEAXAecB5wLXDcVCGWbq5v2G6+Z\nQ5LUX3MGSGZ+BTgyrbwG2FKWtwCXNNW3ZsNuYHFEnAZcCOzMzCOZeRTYCYyXdadk5u7MTGDrtLE6\nmUOS1EcLK/cbycxDZfl7wEhZPh14omm7A6U2W/1Ai3rNHIeYJiLW0/iUwsjICBMTE+39dLM4duxY\nV8bpNvtq3zD2BPbViWHsCXrb17VnH6/et1d91QbIz2RmRkR2o5luz5GZG4GNAKOjozk2NjbvXiYm\nJujGON1mX+0bxp7AvjoxjD1Bb/u6csPt1ftuHl/Uk75q78J6cuq0UXk+XOoHgTOatltWarPVl7Wo\n18whSeqj2gDZDkzdSbUWuK2pfkW5U2oV8Ew5DbUDWB0RS8rF89XAjrLu2YhYVe6+umLaWJ3MIUnq\nozlPYUXEF4AxYGlEHKBxN9WNwK0RsQ74DnBp2fwO4GJgEvghcBVAZh6JiBuAu8t212fm1IX599O4\n0+uVwJ3lQadzSJL6a84AyczLZ1h1QYttE7hmhnE2AZta1PcAb25Rf6rTOSRJ/ePfRJckVTFAJElV\nDBBJUhUDRJJUxQCRJFUxQCRJVQwQSVIVA0SSVMUAkSRVMUAkSVUMEElSFQNEklTFAJEkVTFAJElV\nDBBJUhUDRJJUxQCRJFUxQCRJVQwQSVIVA0SSVMUAkSRVMUAkSVUMEElSFQNEklTFAJEkVTFAJElV\nDBBJUhUDRJJUxQCRJFUxQCRJVQwQSVIVA0SSVMUAkSRVMUAkSVUMEElSFQNEklTFAJEkVTFAJElV\n5hUgEfF4ROyNiPsiYk+pnRoROyNiX3leUuoRETdFxGREPBAR5zSNs7Zsvy8i1jbVV5bxJ8u+Mdsc\nkqT+6cYnkPMz8y2ZOVpebwB2ZeYKYFd5DXARsKI81gM3QyMMgOuA84BzgeuaAuFm4Oqm/cbnmEOS\n1Ce9OIW1BthSlrcAlzTVt2bDbmBxRJwGXAjszMwjmXkU2AmMl3WnZObuzExg67SxWs0hSeqTaPxu\nrtw54jHgKJDAP2bmxoh4OjMXl/UBHM3MxRHx78CNmfnVsm4X8EFgDHhFZn601P8G+BEwUbb/zVJ/\nG/DBzPztmeZo0d96Gp92GBkZWblt27bqn3XKsWPHOPnkk+c9TrfZV/uGsSewr04MY0/Q2772Hnym\net8zX72guq/zzz//nqYzTC+wsLqjhl/PzIMR8TpgZ0T8d/PKzMyIqE+oNsw2R2ZuBDYCjI6O5tjY\n2Lznm5iYoBvjdJt9tW8YewL76sQw9gS97evKDbdX77t5fFFP+prXKazMPFieDwNfonEN48ly+ony\nfLhsfhA4o2n3ZaU2W31ZizqzzCFJ6pPqAImIRRHxqqllYDXwILAdmLqTai1wW1neDlxR7sZaBTyT\nmYeAHcDqiFhSLp6vBnaUdc9GxKpymuqKaWO1mkOS1CfzOYU1Anyp3Fm7EPh8Zv5HRNwN3BoR64Dv\nAJeW7e8ALgYmgR8CVwFk5pGIuAG4u2x3fWYeKcvvBzYDrwTuLA+AG2eYQ5LUJ9UBkpn7gV9pUX8K\nuKBFPYFrZhhrE7CpRX0P8OZ255Ak9Y9/E12SVMUAkSRVMUAkSVUMEElSFQNEklTFAJEkVTFAJElV\nDBBJUhUDRJJUxQCRJFUxQCRJVQwQSVIVA0SSVMUAkSRVMUAkSVUMEElSFQNEklTFAJEkVTFAJElV\nDBBJUhUDRJJUxQCRJFUxQCRJVQwQSVIVA0SSVMUAkSRVMUAkSVUMEElSFQNEklTFAJEkVTFAJElV\nDBBJUhUDRJJUxQCRJFUxQCRJVQwQSVIVA0SSVMUAkSRVWTjoBuYjIsaBvwcWAJ/NzBsH3JKkIbd8\nw+3V+z5+4zu72MmJ74QNkIhYAHwK+C3gAHB3RGzPzIcH25mkucznlzjA5vFFXeqkM3P1fe3Zx7ly\nhm1ejOFzwgYIcC4wmZn7ASJiG7AGMECkPphvCMzH3oPPzPiLelgN8nj1yokcIKcDTzS9PgCc17xB\nRKwH1peXxyLikS7MuxT4fhfG6Tb7at8w9gT21bY/G8KeYHj7Ov/j8+rrDTOtOJEDZE6ZuRHY2M0x\nI2JPZo52c8xusK/2DWNPYF+dGMae4KXX14l8F9ZB4Iym18tKTZLUBydygNwNrIiIMyPiJOAyYPuA\ne5Kkl4wT9hRWZh6PiA8AO2jcxrspMx/qw9RdPSXWRfbVvmHsCeyrE8PYE7zE+orM7MW4kqQXuRP5\nFJYkaYAMEElSFQOkSUSMR8QjETEZERtm2ObSiHg4Ih6KiM831ddGxL7yWDskPT0XEfeVR1dvMJir\nr4j4ZNPcj0bE003renKsutDXII/X6yPiroi4NyIeiIiLm9Z9qOz3SERcOOieImJ5RPyo6Vj9Q7d6\narOvN0TErtLTREQsa1o3yPfWbH315L0VEZsi4nBEPDjD+oiIm0rPD0TEOU3r5n+sMtNH4zrQAuDb\nwM8DJwH3A2dN22YFcC+wpLx+XXk+FdhfnpeU5SWD7KksHxvUsZq2/Z/SuMmhZ8dqvn0N+njRuMj5\nJ2X5LODxpuX7gZcDZ5ZxFgy4p+XAgwM8Vv8CrC3L7wA+NwzvrZn66vF76zeAc2b67wFcDNwJBLAK\n+Ho3j5WfQJ73s69GycyfAFNfjdLsauBTmXkUIDMPl/qFwM7MPFLW7QTGB9xTL7XTV7PLgS+U5V4d\nq/n21Uvt9JXAKWX51cB3y/IaYFtm/jgzHwMmy3iD7KmX2unrLODLZfmupvWDfm/N1FfPZOZXgCOz\nbLIG2JoNu4HFEXEaXTpWBsjzWn01yunTtnkj8MaI+FpE7I7GtwG3u2+/ewJ4RUTsKfVLutBPJ30B\njY/1NP7kPPU/Vq+O1Xz7gsEer48AfxgRB4A7aHw6anfffvcEcGY5tfWfEfG2LvTTSV/3A+8uy+8C\nXhURr2lz30H0Bb17b81lpr67cqwMkM4spHHKaIzGn14/ExGLB9rR7D29IRtfX/AHwN9FxC8MoL/L\ngC9m5nMDmHs2rfoa5PG6HNicmctonHb4XEQM+v/PmXo6BLw+M98K/CXw+Yg4ZZZxuu2vgLdHxL3A\n22l8A8UwvL9m62sY/l/sukG/QYdJO1+NcgDYnpk/LacTHqXxy7tXX6syn57IzIPleT8wAby1Cz21\n29eUy3jhaaJefgXNfPoa9PFaB9xa5v8v4BU0vphvkO+tlj2V02lPlfo9NK4NvLELPbXVV2Z+NzPf\nXQLsw6X2dJs/0yD66uV7ay4z9d2dY9WLCzsn4oPGn+T30zitMXWR7E3TthkHtpTlpTQ+Ar6GxoWo\nx2hcjFpSlk8dcE9LgJc31fcxywXlbvdVtvsl4HHKX1jN5y/edf1YdaGvgR4vGhc6ryzLv0zjekMA\nb+KFF9H3052L6PPp6bVTPdC4qHywn/8Ny3+fl5XljwHXD8N7a5a+evbeKmMuZ+aL6O/khRfRv9HN\nY9WVH+DF8qDxMf1RGn+i+nCpXQ/8blkO4BM0/s2RvcBlTfu+l8YFzkngqkH3BPxqeX1/eV7Xz2NV\nXn8EuLHFvj05VvPpa9DHi8YF2K+V+e8DVjft++Gy3yPARYPuCfg94KFS+ybwO30+Vu+h8Uv4UeCz\nlF/Og35vzdRXL99bND5FHwJ+SuNsxDrgfcD7yvqg8Q/vfbvMPdrNY+VXmUiSqngNRJJUxQCRJFUx\nQCRJVQwQSVIVA0SSVMUAkSRVMUAkSVX+H9ISIibu1pjMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYZ83nfLXvhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aItuYkpo7N-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}